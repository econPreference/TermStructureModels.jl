var documenterSearchIndex = {"docs":
[{"location":"scenario/#Forecasting","page":"Forecasting","title":"Forecasting","text":"","category":"section"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"We have two kinds of forecasts.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Baseline Forecasts\nScenario Forecasts (Scenario Analysis)","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"All of two forecasts are conditional forecasts, because they are based on information in data. The difference is that the scenario forecast assumes additional scenarios that describe future paths of some variables.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Baseline forecasts and scenario forecasts can be represented either as the posterior distribution of predicted objects or as the posterior distribution of conditional expectations of predicted objects. To summarize:","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Posterior Distribution of Predicted Objects\nIn other words, Distribution of future objects conditional on past observations and the scenario\nFunction: conditional_forecasts\nPosterior Distribution of Conditional Expectations of Predicted Objects\nIn other words, Posterior Distribution of \"E[future object|past obs, scenario, parameters]\"\nFunction: scenario_analysis","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"In this summary, for baseline forecasts, the scenario is the empty set.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"The first one is the full Bayesian treatment, so it is mathematically strict. However, it can be difficult to derive meaningful implications from the prediction because of its wide prediction intervals. The second one consider only parameter uncertainty, so it underestimates the prediction uncertainty. However, it is appropriate when users make decisions based on the expected path of future variables. We recommend the second version (scenario_analysis).","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"The required inputs and the type of the output are the same between conditional_forecasts and scenario_analysis. That is,","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"projections = conditional_forecasts(S::Vector, τ, horizon, saved_params, yields, macros, tau_n;\n                                    mean_macros::Vector=[],\n                                    data_scale=1200)","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"and","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"projections = scenario_analysis(S::Vector, τ, horizon, saved_params, yields, macros, tau_n;\n                                mean_macros::Vector=[],\n                                data_scale=1200)","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"projections::Vector{Forecast} contains the results of the forecasting. τ is a vector, and the term premium of τ[i]-bond is forecasted for each i. If τ is set to [], the term premium is not forecasted. horizon is the forecasting horizon. horizon should not be smaller than length(S). saved_params::Vector{Parameter} is the output of posterior_sampler.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Users can use the same yields, tau_n and macros they employed when executing posterior_sampler. If one wishes to compute conditional forecasts using observations up to a certain point, they can simply use yields and macros from the initial period up to that point. However, parameter uncertainty is incorporated independently of yields and macros through saved_params.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"If you use demeaned macro data, option mean_macros is useful. If the sample mean of macro data is specified as the input value for mean_macros, projections contains conditional forecasts of non-demeaned macro variables. The sample mean of macro data can be calculated as follows.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"mean_macros = mean(raw_macros_data, dims=1)[1, :]","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"warning: Option `mean_macros`\nIf macro variables are not demeaned, ignore option mean_macros.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S determines whether we are computing a baseline forecast or a scenario forecast. How S is set will be described in the following sections.","category":"page"},{"location":"scenario/#Baseline-Forecasts","page":"Forecasting","title":"Baseline Forecasts","text":"","category":"section"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Do","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S = []","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"It sets a scenario to an empty set, so the package calculate baseline forecasts.","category":"page"},{"location":"scenario/#Scenario-Forecasts","page":"Forecasting","title":"Scenario Forecasts","text":"","category":"section"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S should be Vector{Scenario}. S can be initialized by","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S = Vector{Scenario}(undef, len)","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"len is the length of S. For example, if the scenario is assumed for the next 5 time periods, len=5.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S[i] represents the scenario for future variables at time T+i, where T refers to the time of the last observation in macros and yields. The type of S[i] is Scenario, and struct Scenario has two fields: combinations::Matrix and values::Vector. The fields in S[i] are implicitly defined by","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S[i].combination*[yields[T+i,:]; macros[T+i, :]] == S[i].values","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"[yields[T+i,:]; macros[T+i, :]] is a predicted variable that is not observed. Scenario forecasts are calculated assuming that the above equation holds at time T+i, based on S[i] set by users. The number of rows in S[i].combination and the length of S[i].values are the same, and this length represents the number of scenarios assumed at time T+i.","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Setting the two fields of S[i] is straightforward. Suppose that the content of the scenarios at time T+i is","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"combs*[yields[T+i,:]; macros[T+i, :]] == vals","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"Then, users can assign the content to S[i] by executing","category":"page"},{"location":"scenario/","page":"Forecasting","title":"Forecasting","text":"S[i] = Scenario(combinations=combs, values=vals)","category":"page"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = TermStructureModels\nDocTestSetup  = quote\n    using TermStructureModels\nend","category":"page"},{"location":"api/#API-documentation","page":"API","title":"API documentation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Exported-Functions","page":"API","title":"Exported Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [TermStructureModels]\nPrivate = false","category":"page"},{"location":"api/#TermStructureModels.Forecast","page":"API","title":"TermStructureModels.Forecast","text":"@kwdef struct Forecast <: PosteriorSample\n\nIt contains a result of the scenario analysis, the conditional prediction for yields, factors = [PCs macros], and term premiums.\n\nyields\nfactors\nTP: term premium forecasts\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Hyperparameter","page":"API","title":"TermStructureModels.Hyperparameter","text":"@kwdef struct Hyperparameter\n\np::Int\nq::Matrix\nnu0\nOmega0::Vector\nmean_phi_const::Vector = zeros(length(Omega0)): It is a prior mean of a constant term in our VAR.\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.LatentSpace","page":"API","title":"TermStructureModels.LatentSpace","text":"@kwdef struct LatentSpace <: PosteriorSample\n\nWhen the model goes to the JSZ latent factor space, the statistical parameters in struct Parameter are also transformed. This struct contains the transformed parameters. Specifically, the transformation is latents[t,:] = T0P_ + inv(T1X)*PCs[t,:]. \n\nIn the latent factor space, the transition equation is data[t,:] = KPXF + GPXFXF*vec(data[t-1:-1:t-p,:]') + MvNormal(O,OmegaXFXF), where data = [latent macros].\n\nlatents::Matrix\nkappaQ\nkQ_infty\nKPXF::Vector\nGPXFXF::Matrix\nOmegaXFXF::Matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Parameter","page":"API","title":"TermStructureModels.Parameter","text":"@kwdef struct Parameter <: PosteriorSample\n\nIt contains statistical parameters of the model that are sampled from function posterior_sampler.\n\nkappaQ\nkQ_infty::Float64\nphi::Matrix{Float64}\nvarFF::Vector{Float64}\nSigmaO::Vector{Float64}\ngamma::Vector{Float64}\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.PosteriorSample","page":"API","title":"TermStructureModels.PosteriorSample","text":"abstract type PosteriorSample\n\nIt is a super-set of structs Parameter, ReducedForm, LatentSpace, YieldCurve, TermPremium, Forecast.\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.ReducedForm","page":"API","title":"TermStructureModels.ReducedForm","text":"@kwdef struct ReducedForm <: PosteriorSample\n\nIt contains statistical parameters in terms of the reduced form VAR(p) in P-dynamics. lambdaP and LambdaPF are parameters in the market prices of risks equation, and they only contain the first dQ non-zero equations. \n\nkappaQ\nkQ_infty\nKPF\nGPFF\nOmegaFF::Matrix\nSigmaO::Vector\nlambdaP\nLambdaPF\nmpr::Matrix(market prices of risks, T, dP)\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Scenario","page":"API","title":"TermStructureModels.Scenario","text":"@kwdef struct Scenario\n\nIt contains scenarios to be conditioned in the scenario analysis. When y = [yields; macros] is a observed vector in our measurement equation, Scenario.combinations*y = Scenario.values constitutes the scenario at a specific time. Vector{Scenario} is used to describe a time-series of scenarios.\n\ncombinations and values should be Matrix and Vector. If values is a scalar, combinations would be a matrix with one raw vector and values should be one-dimensional vector, for example [values]. \n\ncombinations::Matrix\nvalues::Vector\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.TermPremium","page":"API","title":"TermStructureModels.TermPremium","text":"@kwdef struct TermPremium <: PosteriorSample\n\nThe yields are decomposed into the term premium(TP) and the expectation hypothesis component(EH). Each components have constant terms(const_TP and const_EH) and time-varying components(timevarying_TP and timevarying_EH). factorloading_EH and factorloading_TP are coefficients of the pricing factors for the time varying components. Each column of the outputs indicates the results for each maturity.\n\nWe do not store time-varying components in TermPremium, and the time-varying components are the separate outputs in function term_premium. \n\nTP\nEH\nfactorloading_TP\nfactorloading_EH\nconst_TP\nconst_EH\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.YieldCurve","page":"API","title":"TermStructureModels.YieldCurve","text":"@kwdef struct YieldCurve <: PosteriorSample\n\nIt contains a fitted yield curve. yields[t,:] = intercept + slope*latents[t,:] holds.\n\nlatents::Matrix: latent pricing factors in LatentSpace\nyields\nintercept\nslope\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.getindex-Tuple{PosteriorSample, Symbol}","page":"API","title":"Base.getindex","text":"getindex(x::PosteriorSample, c::Symbol)\n\nFor struct <: PosteriorSample, struct[:name] calls objects in struct.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.getindex-Tuple{Vector{var\"#s13\"} where var\"#s13\"<:PosteriorSample, Symbol}","page":"API","title":"Base.getindex","text":"getindex(x::Vector{<:PosteriorSample}, c::Symbol)\n\nFor struct <: PosteriorSample, struct[:name] calls objects in struct. Output[i] = ith posterior sample\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{Vector{var\"#s12\"} where var\"#s12\"<:PosteriorSample}","page":"API","title":"Statistics.mean","text":"mean(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior mean.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.median-Tuple{Vector{var\"#s7\"} where var\"#s7\"<:PosteriorSample}","page":"API","title":"Statistics.median","text":"median(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior median.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.quantile-Tuple{Vector{var\"#s7\"} where var\"#s7\"<:PosteriorSample, Any}","page":"API","title":"Statistics.quantile","text":"quantile(x::Vector{<:PosteriorSample}, q)\n\nOutput[:variable name] returns a quantile of the corresponding posterior distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.std-Tuple{Vector{var\"#s12\"} where var\"#s12\"<:PosteriorSample}","page":"API","title":"Statistics.std","text":"std(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior standard deviation.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{Vector{var\"#s12\"} where var\"#s12\"<:PosteriorSample}","page":"API","title":"Statistics.var","text":"var(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior variance.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.AR_res_var-Tuple{Vector{T} where T, Any}","page":"API","title":"TermStructureModels.AR_res_var","text":"AR_res_var(TS::Vector, p)\n\nIt derives an MLE error variance estimate of an AR(p) model\n\nInput\n\nunivariate time series TS and the lag p\n\noutput(2)\n\nresidual variance estimate, AR(p) coefficients\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.GQ_XX-Tuple{}","page":"API","title":"TermStructureModels.GQ_XX","text":"GQ_XX(; kappaQ)\n\nkappaQ governs a conditional mean of the Q-dynamics of X, and its slope matrix has a restricted form. This function shows that restricted form.\n\nOutput\n\nslope matrix of the Q-conditional mean of X\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.LDL-Tuple{Any}","page":"API","title":"TermStructureModels.LDL","text":"LDL(X)\n\nThis function generate a matrix decomposition, called LDLt. X = L*D*L', where L is a lower triangular matrix and D is a diagonal. How to conduct it can be found at Wikipedia.\n\nInput\n\nDecomposed Object, X\n\nOutput(2)\n\nL, D\n\nDecomposed result is X = L*D*L'\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.PCA","page":"API","title":"TermStructureModels.PCA","text":"PCA(yields, p, proxies=[]; rescaling=false, dQ=[])\n\nIt derives the principal components from yields.\n\nInput\n\nyields[p+1:end, :] is used to construct the affine transformation, and then all yields[:,:] are transformed into the principal components.\nSince signs of PCs is not identified, we use proxies to identify the signs. We flip PCs to make cor(proxies[:, i]. PCs[:,i]) > 0. If proxies is not given, we use the following proxies as a default: [yields[:, end] yields[:, end] - yields[:, 1] 2yields[:, Int(floor(size(yields, 2) / 3))] - yields[:, 1] - yields[:, end]].\nsize(proxies) = (size(yields[p+1:end, :], 1), dQ)\nIf rescaling == true, all PCs and OCs are normalized to have an average std of yields.\n\nOutput(4)\n\nPCs, OCs, Wₚ, Wₒ, mean_PCs\n\nPCs, OCs: first dQ and the remaining principal components\nWₚ, Wₒ: the rotation matrix for PCs and OCs, respectively\nmean_PCs: the mean of PCs before demeaned.\nPCs are demeaned.\n\n\n\n\n\n","category":"function"},{"location":"api/#TermStructureModels.calibrate_mean_phi_const-NTuple{7, Any}","page":"API","title":"TermStructureModels.calibrate_mean_phi_const","text":"calibrate_mean_phi_const(mean_kQ_infty, std_kQ_infty, nu0, yields, macros, tau_n, p; mean_phi_const_PCs=[], medium_tau=collect(24:3:48), iteration=1000, data_scale=1200, kappaQ_prior_pr=[], τ=[])\n\nThe purpose of the function is to calibrate a prior mean of the first dQ constant terms in our VAR. Adjust your prior setting based on the prior samples in outputs.\n\nInput\n\nmean_phi_const_PCs is your prior mean of the first dQ constants. Our default option set it as a zero vector.\niteration is the number of prior samples.\nτ::scalar is a maturity for calculating the constant part in the term premium.\nIf τ is empty, the function does not sampling the prior distribution of the constant part in the term premium.\n\nOutput(2)\n\nprior_λₚ, prior_TP\n\nsamples from the prior distribution of λₚ \nprior samples of constant part in the τ-month term premium\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.conditional_forecasts-Tuple{Vector{T} where T, Any, Any, Any, Any, Any, Any}","page":"API","title":"TermStructureModels.conditional_forecasts","text":"conditional_forecasts(S::Vector, τ, horizon, saved_params, yields, macros, tau_n; mean_macros::Vector=[], data_scale=1200)\n\nInput\n\nscenarios, a result of the posterior sampler, and data \n\nS[t] = conditioned scenario at time size(yields, 1)+t.\nIf we need an unconditional prediction, S = [].\nIf you are conditionaing a scenario, I assume S = Vector{Scenario}.\nτ is a vector. The term premium of τ[i]-bond is forecasted for each i.\nIf τ is set to [], the term premium is not forecasted. \nhorizon: maximum length of the predicted path. It should not be small than length(S).\nsaved_params: the first output of function posterior_sampler.\nmean_macros::Vector: If you demeaned macro variables, you can input the mean of the macro variables. Then, the output will be generated in terms of the un-demeaned macro variables.\n\nOutput\n\nVector{Forecast}(, iteration)\nt'th rows in predicted yields, predicted factors, and predicted TP are the corresponding predicted value at time size(yields, 1)+t.\nMathematically, it is a posterior samples from future observation|past observation,scenario.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.dcurvature_dτ-Tuple{Any}","page":"API","title":"TermStructureModels.dcurvature_dτ","text":"dcurvature_dτ(τ; kappaQ)\n\nThis function calculate the first derivative of the curvature factor loading w.r.t. the maturity.\n\nInput\n\nkappaQ: The decay parameter\nτ: The maturity that the derivative is calculated\n\nOutput\n\nthe first derivative of the curvature factor loading w.r.t. the maturity\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.dimQ-Tuple{}","page":"API","title":"TermStructureModels.dimQ","text":"dimQ()\n\nIt returns the dimension of Q-dynamics under the standard ATSM.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.erase_nonstationary_param-Tuple{Any}","page":"API","title":"TermStructureModels.erase_nonstationary_param","text":"erase_nonstationary_param(saved_params; threshold=1)\n\nIt filters out posterior samples that implies an unit root VAR system. Only stationary posterior samples remain.\n\nInput\n\nsaved_params is the first output of function posterior_sampler.\nPosterior samples with eigenvalues of the P-system greater than threshold are removed. \n\nOutput(2):\n\nstationary samples, acceptance rate(%)\n\nThe second output indicates how many posterior samples remain.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.fitted_YieldCurve-Tuple{Any, Vector{LatentSpace}}","page":"API","title":"TermStructureModels.fitted_YieldCurve","text":"fitted_YieldCurve(τ0, saved_latent_params::Vector{LatentSpace}; data_scale=1200)\n\nIt generates a fitted yield curve.\n\nInput\n\nτ0 is a set of maturities of interest. τ0 does not need to be the same as the one used for the estimation.\nsaved_latent_params is a transformed posterior sample using function latentspace.\n\nOutput\n\nVector{YieldCurve}(,# of iteration)\nyields and latents contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.generative-Tuple{Any, Any, Any, Any, Float64}","page":"API","title":"TermStructureModels.generative","text":"generative(T, dP, tau_n, p, noise::Float64; kappaQ, kQ_infty, KPXF, GPXFXF, OmegaXFXF, data_scale=1200)\n\nThis function generate a simulation data given parameters. Note that all parameters are the things in the latent factor state space (that is, parameters in struct LatentSpace). There is some differences in notations because it is hard to express mathcal letters in VScode. So, mathcal{F} in my paper is expressed in F in the VScode. And, \"F\" in my paper is expressed as XF.\n\nInput:\n\nnoise = variance of the measurement errors\n\nOutput(3)\n\nyields, latents, macros\n\nyields = Matrix{Float64}(obs,T,length(tau_n))\nlatents = Matrix{Float64}(obs,T,dimQ())\nmacros = Matrix{Float64}(obs,T,dP - dimQ())\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.hessian","page":"API","title":"TermStructureModels.hessian","text":"hessian(f, x, index=[])\n\nIt calculates the Hessian matrix of a scalar function f at x. If index is not empty, it calculates the Hessian matrix of the function with respect to the selected variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#TermStructureModels.ineff_factor-Tuple{Any}","page":"API","title":"TermStructureModels.ineff_factor","text":"ineff_factor(saved_params)\n\nIt returns inefficiency factors of each parameter.\n\nInput\n\nVector{Parameter} from posterior_sampler\n\nOutput\n\nEstimated inefficiency factors are in Tuple(kappaQ, kQ_infty, gamma, SigmaO, varFF, phi). For example, if you want to load an inefficiency factor of phi, you can use Output.phi.\nIf fix_const_PC1==true in your optimized struct Hyperparameter, Output.phi[1,1] can be weird. So you should ignore it.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.isstationary-Tuple{Any}","page":"API","title":"TermStructureModels.isstationary","text":"isstationary(GPFF; threshold)\n\nIt checks whether a reduced VAR matrix has unit roots. If there is at least one unit root, return is false.\n\nInput\n\nGPFF should not include intercepts. Also, GPFF is dP by dP*p matrix that the coefficient at lag 1 comes first, and the lag p slope matrix comes last. \nPosterior samples with eigenvalues of the P-system greater than threshold are removed. Typically, threshold is set to 1.\n\nOutput\n\nboolean\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.latentspace-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.latentspace","text":"latentspace(saved_params, yields, tau_n; data_scale=1200)\n\nThis function translates the principal components state space into the latent factor state space. \n\nInput\n\ndata_scale::scalar: In typical affine term structure model, theoretical yields are in decimal and not annualized. But, for convenience(public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput\n\nVector{LatentSpace}(, iteration)\nlatent factors contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.log_marginal-Tuple{Any, Any, Any, Hyperparameter, Any, Any}","page":"API","title":"TermStructureModels.log_marginal","text":"log_marginal(PCs, macros, rho, tuned::Hyperparameter, tau_n, Wₚ; psi=[], psi_const=[], medium_tau, kappaQ_prior_pr, fix_const_PC1)\n\nThis file calculates a value of our marginal likelihood. Only the transition equation is used to calculate it. \n\nInput\n\ntuned is a point where the marginal likelihood is evaluated. \t\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. A empty default value means that you do not use this function. [psi_const psi][i,j] is corresponds to phi[i,j]. \n\nOutput\n\nthe log marginal likelihood of the VAR system.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_mea-Tuple{Any, Any}","page":"API","title":"TermStructureModels.loglik_mea","text":"loglik_mea(yields, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale)\n\nThis function generate a log likelihood of the measurement equation.\n\nOutput\n\nthe measurement equation part of the log likelihood\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_tran-Tuple{Any, Any}","page":"API","title":"TermStructureModels.loglik_tran","text":"loglik_tran(PCs, macros; phi, varFF)\n\nIt calculate log likelihood of the transition equation. \n\nOutput\n\nlog likelihood of the transition equation.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_2_phi₀_C-Tuple{}","page":"API","title":"TermStructureModels.phi_2_phi₀_C","text":"phi_2_phi₀_C(; phi)\n\nIt divide phi into the lagged regressor part and the contemporaneous regerssor part.\n\nOutput(3)\n\nphi0, C = C0 + I, C0\n\nphi0: coefficients for the lagged regressors\nC: coefficients for the dependent variables when all contemporaneous variables are in the LHS of the orthogonalized equations. Therefore, the diagonals of C is ones. Note that since the contemporaneous variables get negative signs when they are at the RHS, the signs of C do not change whether they are at the RHS or LHS. \n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_varFF_2_OmegaFF-Tuple{}","page":"API","title":"TermStructureModels.phi_varFF_2_OmegaFF","text":"phi_varFF_2_OmegaFF(; phi, varFF)\n\nIt construct OmegaFF from statistical parameters.\n\nOutput\n\nOmegaFF\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.posterior_sampler-Tuple{Any, Any, Any, Any, Any, Hyperparameter}","page":"API","title":"TermStructureModels.posterior_sampler","text":"posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter; medium_tau=collect(24:3:48), init_param=[], psi=[], psi_const=[], gamma_bar=[], kappaQ_prior_pr=[], mean_kQ_infty=0, std_kQ_infty=0.1, fix_const_PC1=false, data_scale=1200)\n\nThis is a posterior distribution sampler.\n\nInput\n\niteration: # of posterior samples\ntuned: optimized hyperparameters used during estimation\ninit_param: starting point of the sampler. It should be a type of Parameter.\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. A empty default value means that you do not use this function. [psi_const psi][i,j] is corresponds to phi[i,j]. The entries of psi and psi_const should be nearly zero(e.g., 1e-10), not exactly zero.\n\nOutput(2)\n\nVector{Parameter}(posterior, iteration), acceptance rate of the MH algorithm\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_kappaQ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.prior_kappaQ","text":"prior_kappaQ(medium_tau, pr)\n\nThe function derive the maximizer decay parameter kappaQ that maximize the curvature factor loading at each candidate medium-term maturity. And then, it impose a discrete prior distribution on the maximizers with a prior probability vector pr.\n\nInput\n\nmedium_tau::Vector(candidate medium maturities, # of candidates)\npr::Vector(probability, # of candidates)\n\nOutput\n\ndiscrete prior distribution that has a support of the maximizers kappaQ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.reducedform-NTuple{4, Any}","page":"API","title":"TermStructureModels.reducedform","text":"reducedform(saved_params, yields, macros, tau_n; data_scale=1200)\n\nIt converts posterior samples in terms of the reduced form VAR parameters.\n\nInput\n\nsaved_params is the first output of function posterior_sampler.\n\nOutput\n\nPosterior samples in terms of struct ReducedForm\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.scenario_analysis-Tuple{Vector{T} where T, Any, Any, Any, Any, Any, Any}","page":"API","title":"TermStructureModels.scenario_analysis","text":"scenario_analysis(S::Vector, τ, horizon, saved_params, yields, macros, tau_n; mean_macros::Vector=[], data_scale=1200)\n\nInput\n\nscenarios, a result of the posterior sampler, and data \n\nS[t] = conditioned scenario at time size(yields, 1)+t.\nSet S = [] if you need an unconditional prediction. \nIf you are conditionaing a scenario, I assume S = Vector{Scenario}.\nτ is a vector of maturities that term premiums of interest has.\nhorizon: maximum length of the predicted path. It should not be small than length(S).\nsaved_params: the first output of function posterior_sampler.\nmean_macros::Vector: If you demeaned macro variables, you can input the mean of the macro variables. Then, the output will be generated in terms of the un-demeaned macro variables.\n\nOutput\n\nVector{Forecast}(, iteration)\nt'th rows in predicted yields, predicted factors, and predicted TP are the corresponding predicted value at time size(yields, 1)+t.\nMathematically, it is a posterior distribution of E[future obs|past obs, scenario, parameters].\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.term_premium-NTuple{5, Any}","page":"API","title":"TermStructureModels.term_premium","text":"term_premium(tau_interest, tau_n, saved_params, yields, macros; data_scale=1200)\n\nThis function generates posterior samples of the term premiums.\n\nInput\n\nmaturity of interest tau_interest for Calculating TP\nsaved_params from function posterior_sampler\n\nOutput(3)\n\nsaved_TP, saved_tv_TP, saved_tv_EH\n\nsaved_TP::Vector{TermPremium}(, iteration)\nsaved_tv_TP::Vector{Array}(, iteration)\nsaved_tv_EH::Vector{Array}(, iteration)\nBoth the term premiums and expectation hypothesis components are decomposed into the time-invariant part and time-varying part. For the maturity tau_interest[i] and j-th posterior sample, the time-varying parts are saved in saved_tv_TP[j][:, :, i] and saved_tv_EH[j][:, :, i]. The time-varying parts driven by the k-th pricing factor is stored in saved_tv_TP[j][:, k, i] and saved_tv_EH[j][:, k, i].\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.tuning_hyperparameter-NTuple{4, Any}","page":"API","title":"TermStructureModels.tuning_hyperparameter","text":"tuning_hyperparameter(yields, macros, tau_n, rho; populationsize=50, maxiter=10_000, medium_tau=collect(24:3:48), upper_q=[1 1; 1 1; 10 10; 100 100], mean_kQ_infty=0, std_kQ_infty=0.1, upper_nu0=[], mean_phi_const=[], fix_const_PC1=false, upper_p=18, mean_phi_const_PC1=[], data_scale=1200, kappaQ_prior_pr=[], init_nu0=[], is_pure_EH=false, psi_common=[], psi_const=[])\n\nIt optimizes our hyperparameters by maximizing the marginal likelhood of the transition equation. Our optimizer is a differential evolutionary algorithm that utilizes bimodal movements in the eigen-space(Wang, Li, Huang, and Li, 2014) and the trivial geography(Spector and Klein, 2006).\n\nInput\n\nWhen we compare marginal likelihoods between models, the data for the dependent variable should be the same across the models. To achieve that, we set a period of dependent variable based on upper_p. For example, if upper_p = 3, yields[4:end,:] and macros[4:end,:] are the data for our dependent variable. yields[1:3,:] and macros[1:3,:] are used for setting initial observations for all lags.\npopulationsize and maxiter are options for the optimizer.\npopulationsize: the number of candidate solutions in each generation\nmaxtier: the maximum number of iterations\nThe lower bounds for q and nu0 are 0 and dP+2. \nThe upper bounds for q, nu0 and VAR lag can be set by upper_q, upper_nu0, upper_p.\nOur default option for upper_nu0 is the time-series length of the data.\nIf you use our default option for mean_phi_const,\nmean_phi_const[dQ+1:end] is a zero vector.\nmean_phi_const[1:dQ] is calibrated to make a prior mean of λₚ a zero vector.\nAfter step 2, mean_phi_const[1] is replaced with mean_phi_const_PC1 if it is not empty.\nmean_phi_const = Matrix(your prior, dP, upper_p) \nmean_phi_const[:,i] is a prior mean for the VAR(i) constant. Therefore mean_phi_const is a matrix only in this function. In other functions, mean_phi_const is a vector for the orthogonalized VAR system with your selected lag.\nWhen fix_const_PC1==true, the first element in a constant term in our orthogonalized VAR is fixed to its prior mean during the posterior sampling.\ndata_scale::scalar: In typical affine term structure model, theoretical yields are in decimal and not annualized. But, for convenience(public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\nis_pure_EH::Bool: When mean_phi_const=[], is_pure_EH=false sets mean_phi_const to zero vectors. Otherwise, mean_phi_const is set to imply the pure expectation hypothesis under mean_phi_const=[].\npsi_const and psi = kron(ones(1, lag length), psi_common) are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. A empty default value means that you do not use this function. [psi_const psi][i,j] is corresponds to phi[i,j]. The entries of psi_common and psi_const should be nearly zero(e.g., 1e-10), not exactly zero.\n\nOutput(2)\n\noptimized Hyperparameter, optimization result\n\nBe careful that we minimized the negative log marginal likelihood, so the second output is about the minimization problem.\n\n\n\n\n\n","category":"method"},{"location":"api/#Internal-Functions","page":"API","title":"Internal Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [TermStructureModels]\nPublic = false","category":"page"},{"location":"api/#TermStructureModels.Aₓ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.Aₓ","text":"Aₓ(aτ_, tau_n)\n\nInput\n\naτ_ is an output of function aτ.\n\nOutput\n\nAₓ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Aₚ-NTuple{4, Any}","page":"API","title":"TermStructureModels.Aₚ","text":"Aₚ(Aₓ_, Bₓ_, T0P_, Wₒ)\n\nInput\n\nAₓ_, Bₓ_, and T0P_ are outputs of function Aₓ, Bₓ, and T0P, respectively.\n\nOutput\n\nAₚ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Bₓ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.Bₓ","text":"Bₓ(bτ_, tau_n)\n\nInput\n\nbτ_ is an output of function bτ.\n\nOutput\n\nBₓ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Bₚ-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.Bₚ","text":"Bₚ(Bₓ_, T1X_, Wₒ)\n\nInput\n\nBₓ_ and T1X_ are outputs of function Bₓ and T1X, respectively.\n\nOutput\n\nBₚ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Kphi-NTuple{4, Any}","page":"API","title":"TermStructureModels.Kphi","text":"Kphi(i, V, Xphi, dP)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Minnesota-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.Minnesota","text":"Minnesota(l, i, j; q, nu0, Omega0, dQ=[])\n\nIt return unscaled prior variance of the Minnesota prior.\n\nInput\n\nlag l, dependent variable i, regressor j in the VAR(p)\nq[:,1] and q[:,2] are [own, cross, lag, intercept] shrikages for the first dQ and remaining dP-dQ equations, respectively.\nnu0(d.f.), Omega0(scale): Inverse-Wishart prior for the error-covariance matrix of VAR(p).\n\nOutput\n\nMinnesota part in the prior variance\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.NIG_NIG-NTuple{6, Any}","page":"API","title":"TermStructureModels.NIG_NIG","text":"NIG_NIG(y, X, β₀, B₀, α₀, δ₀)\n\nNormal-InverseGamma-Normal-InverseGamma update\n\nprior: β|σ² ~ MvNormal(β₀,σ²B₀), σ² ~ InverseGamma(α₀,δ₀)\nlikelihood: y|β,σ² = Xβ + MvNormal(zeros(T,1),σ²I(T))\n\nOutput(2)\n\nβ, σ²\n\nposterior sample\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.PCs_2_latents-Tuple{Any, Any}","page":"API","title":"TermStructureModels.PCs_2_latents","text":"PCs_2_latents(yields, tau_n; kappaQ, kQ_infty, KPF, GPFF, OmegaFF, data_scale)\n\nNotation XF is for the latent factor space and notation F is for the PC state space.\n\nInput\n\ndata_scale::scalar: In typical affine term structure model, theoretical yields are in decimal and not annualized. But, for convenience(public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput(6)\n\nlatent, kappaQ, kQ_infty, KPXF, GPXFXF, OmegaXFXF\n\nlatent factors contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.S-Tuple{Any}","page":"API","title":"TermStructureModels.S","text":"S(i; Omega0)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.S_hat-NTuple{6, Any}","page":"API","title":"TermStructureModels.S_hat","text":"S_hat(i, m, V, yphi, Xphi, dP; Omega0)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.T0P-NTuple{4, Any}","page":"API","title":"TermStructureModels.T0P","text":"T0P(T1X_, Aₓ_, Wₚ, c)\n\nInput\n\nT1X_ and Aₓ_ are outputs of function T1X and Aₓ, respectively. c is a sample mean of PCs.\n\nOutput\n\nT0P\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.T1X-Tuple{Any, Any}","page":"API","title":"TermStructureModels.T1X","text":"T1X(Bₓ_, Wₚ)\n\nInput\n\nBₓ_ if an output of function Bₓ.\n\nOutput\n\nT1X\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._conditional_forecasts-NTuple{6, Any}","page":"API","title":"TermStructureModels._conditional_forecasts","text":"_conditional_forecasts(S, τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._scenario_analysis-NTuple{6, Any}","page":"API","title":"TermStructureModels._scenario_analysis","text":"_scenario_analysis(S, τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._scenario_analysis_unconditional-NTuple{5, Any}","page":"API","title":"TermStructureModels._scenario_analysis_unconditional","text":"_scenario_analysis_unconditional(τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._termPremium-NTuple{6, Any}","page":"API","title":"TermStructureModels._termPremium","text":"_termPremium(τ, PCs, macros, bτ_, T0P_, T1X_; kappaQ, kQ_infty, KPF, GPFF, ΩPP, data_scale)\n\nThis function calculates a term premium for maturity τ. \n\nInput\n\ndata_scale::scalar = In typical affine term structure model, theoretical yields are in decimal and not annualized. But, for convenience(public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput(4)\n\nTP, timevarying_TP, const_TP, jensen\n\nTP: term premium of maturity τ\ntimevarying_TP: contributions of each [PCs macros] on TP at each time t (row: time, col: variable)\nconst_TP: Constant part of TP\njensen: Jensen's Ineqaulity part in TP\nOutput excludes the time period for the initial observations.  \n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._unconditional_forecasts-NTuple{5, Any}","page":"API","title":"TermStructureModels._unconditional_forecasts","text":"_unconditional_forecasts(τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.aτ-NTuple{4, Any}","page":"API","title":"TermStructureModels.aτ","text":"aτ(N, bτ_, tau_n, Wₚ; kQ_infty, ΩPP, data_scale)\naτ(N, bτ_; kQ_infty, ΩXX, data_scale)\n\nThe function has two methods(multiple dispatch). \n\nInput\n\nWhen Wₚ ∈ arguments: It calculates aτ using ΩPP. \nOtherwise: It calculates aτ using ΩXX = OmegaXFXF[1:dQ, 1:dQ], so parameters are in the latent factor space. So, we do not need Wₚ.\nbτ_ is an output of function bτ.\ndata_scale::scalar: In typical affine term structure model, theoretical yields are in decimal and not annualized. But, for convenience(public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput\n\nVector(Float64)(aτ,N)\nFor i'th maturity, Output[i] is the corresponding aτ.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.bτ-Tuple{Any}","page":"API","title":"TermStructureModels.bτ","text":"bτ(N; kappaQ, dQ)\n\nIt solves the difference equation for bτ.\n\nOutput\n\nfor maturity i, bτ[:, i] is a vector of factor loadings.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.jensens_inequality-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.jensens_inequality","text":"jensens_inequality(τ, bτ_, T1X_; ΩPP, data_scale)\n\nThis function evaluate the Jensen's Ineqaulity term. All term is invariant with respect to the data_scale, except for this Jensen's inequality term. So, we need to scale down the term by data_scale.\n\nOutput\n\nJensen's Ineqaulity term for aτ of maturity τ.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_mea2-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.loglik_mea2","text":"loglik_mea2(yields, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale)\n\nThis function is the same as loglik_mea but it requires ΩPP as an input.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.longvar-Tuple{Any}","page":"API","title":"TermStructureModels.longvar","text":"longvar(v)\n\nIt calculates the long-run variance of v using the quadratic spectral window with selection of bandwidth of Andrews(1991). We use the AR(1) approximation.\n\nInput\n\nTime-series Vector v\n\nOutput\n\nEstimated 2πh(0) of v, where h(x) is the spectral density of v at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.mle_error_covariance-NTuple{4, Any}","page":"API","title":"TermStructureModels.mle_error_covariance","text":"mle_error_covariance(yields, macros, tau_n, p)\n\nIt calculates the MLE estimates of the error covariance matrix of the VAR(p) model.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_hat-NTuple{6, Any}","page":"API","title":"TermStructureModels.phi_hat","text":"phi_hat(i, m, V, yphi, Xphi, dP)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_varFF_2_ΩPP-Tuple{}","page":"API","title":"TermStructureModels.phi_varFF_2_ΩPP","text":"phi_varFF_2_ΩPP(; phi, varFF, dQ=[])\n\nIt construct ΩPP from statistical parameters.\n\nOutput\n\nΩPP\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_SigmaO-Tuple{Any, Any}","page":"API","title":"TermStructureModels.post_SigmaO","text":"post_SigmaO(yields, tau_n; kappaQ, kQ_infty, ΩPP, gamma, p, data_scale)\n\nPosterior sampler for the measurement errors\n\nOutput\n\nVector{Dist}(IG, N-dQ)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_gamma-Tuple{}","page":"API","title":"TermStructureModels.post_gamma","text":"post_gamma(; gamma_bar, SigmaO)\n\nPosterior sampler for the population measurement error\n\nOutput\n\nVector{Dist}(Gamma,length(SigmaO))\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kQ_infty-NTuple{4, Any}","page":"API","title":"TermStructureModels.post_kQ_infty","text":"post_kQ_infty(mean_kQ_infty, std_kQ_infty, yields, tau_n; kappaQ, phi, varFF, SigmaO, data_scale)\n\nOutput\n\nFull conditional posterior distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kappaQ-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.post_kappaQ","text":"post_kappaQ(yields, prior_kappaQ_, tau_n; kQ_infty, phi, varFF, SigmaO, data_scale)\n\nInput\n\nprior_kappaQ_ is a output of function prior_kappaQ.\n\nOutput\n\nFull conditional posterior distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kappaQ2-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.post_kappaQ2","text":"post_kappaQ2(yields, prior_kappaQ_, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale, x_mode, inv_x_hess)\n\nIt conducts the Metropolis-Hastings algorithm for the reparameterized kappaQ under the unrestricted JSZ form. x_mode and inv_x_hess constitute the mean and variance of the Normal proposal distribution.\n\nReparameterization:   kappaQ[1] = x[1]   kappaQ[2] = x[1] + x[2]   kappaQ[3] = x[1] + x[2] + x[3]\nJacobian:   [1 0 0   1 1 0   1 1 1]\nThe determinant = 1\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_phi_varFF-NTuple{6, Any}","page":"API","title":"TermStructureModels.post_phi_varFF","text":"post_phi_varFF(yields, macros, mean_phi_const, rho, prior_kappaQ_, tau_n; phi, psi, psi_const, varFF, q, nu0, Omega0, kappaQ, kQ_infty, SigmaO, fix_const_PC1, data_scale)\n\nFull-conditional posterior sampler for phi and varFF \n\nInput\n\nprior_kappaQ_ is a output of function prior_kappaQ.\nWhen fix_const_PC1==true, the first element in a constant term in our orthogonalized VAR is fixed to its prior mean during the posterior sampling.\n\nOutput(3)\n\nphi, varFF, isaccept=Vector{Bool}(undef, dQ)\n\nIt gives a posterior sample.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_C-Tuple{}","page":"API","title":"TermStructureModels.prior_C","text":"prior_C(; Omega0::Vector)\n\nWe translate the Inverse-Wishart prior to a series of the Normal-Inverse-Gamma (NIG) prior distributions. If the dimension is dₚ, there are dₚ NIG prior distributions. This function generates Normal priors.  \n\nOutput:\n\nunscaled prior of C in the LDLt decomposition, OmegaFF = inv(C)*diagm(varFF)*inv(C)'\n\nImportant note\n\nprior variance for C[i,:] = varFF[i]*variance of output[i,:]\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_gamma-Tuple{Any, Any}","page":"API","title":"TermStructureModels.prior_gamma","text":"prior_gamma(yields, p)\n\nThere is a hierarchcal structure in the measurement equation. The prior means of the measurement errors are gamma[i] and each gamma[i] follows Gamma(1,gamma_bar) distribution. This function decides gamma_bar empirically. OLS is used to estimate the measurement equation and then a variance of residuals is calculated for each maturities. An inverse of the average residual variances is set to gamma_bar.\n\nOutput\n\nhyperparameter gamma_bar\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_phi0-Tuple{Any, Vector{T} where T, Any, Any, Any}","page":"API","title":"TermStructureModels.prior_phi0","text":"prior_phi0(mean_phi_const, rho::Vector, prior_kappaQ_, tau_n, Wₚ; psi_const, psi, q, nu0, Omega0, fix_const_PC1)\n\nThis part derives the prior distribution for coefficients of the lagged regressors in the orthogonalized VAR. \n\nInput\n\nprior_kappaQ_ is a output of function prior_kappaQ.\nWhen fix_const_PC1==true, the first element in a constant term in our orthogonalized VAR is fixed to its prior mean during the posterior sampling.\n\nOutput\n\nNormal prior distributions on the slope coefficient of lagged variables and intercepts in the orthogonalized equation. \nOutput[:,1] for intercepts, Output[:,1+1:1+dP] for the first lag, Output[:,1+dP+1:1+2*dP] for the second lag, and so on.\n\nImportant note\n\nprior variance for phi[i,:] = varFF[i]*var(output[i,:])\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_varFF-Tuple{}","page":"API","title":"TermStructureModels.prior_varFF","text":"prior_varFF(; nu0, Omega0::Vector)\n\nWe translate the Inverse-Wishart prior to a series of the Normal-Inverse-Gamma (NIG) prior distributions. If the dimension is dₚ, there are dₚ NIG prior distributions. This function generates Inverse-Gamma priors.  \n\nOutput:\n\nprior of varFF in the LDLt decomposition,OmegaFF = inv(C)*diagm(varFF)*inv(C)'\nEach element in the output follows Inverse-Gamma priors.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.yphi_Xphi-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.yphi_Xphi","text":"yphi_Xphi(PCs, macros, p)\n\nThis function generate the dependent variable and the corresponding regressors in the orthogonalized transition equation.\n\nOutput(4)\n\nyphi, Xphi = [ones(T - p) Xphi_lag Xphi_contemporaneous], [ones(T - p) Xphi_lag], Xphi_contemporaneous\n\nyphi and Xphi is a full matrix. For i'th equation, the dependent variable is yphi[:,i] and the regressor is Xphi. \nXphi is same to all orthogonalized transtion equations. The orthogonalized equations are different in terms of contemporaneous regressors. Therefore, the corresponding regressors in Xphi should be excluded. The form of parameter phi do that task by setting the coefficients of the excluded regressors to zeros. In particular, for last dP by dP block in phi, the diagonals and the upper diagonal elements should be zero. \n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.ν-Tuple{Any, Any}","page":"API","title":"TermStructureModels.ν","text":"ν(i, dP; nu0)\n\n\n\n\n\n","category":"method"},{"location":"others/#Other-Forms-of-the-Model","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"","category":"section"},{"location":"others/#Yield-Only-Model","page":"Other Forms of the Model","title":"Yield-Only Model","text":"","category":"section"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"Users may want to use yield-only models in which macros is an empty set. In such instances, set macros = [] and rho = [] for all functions.","category":"page"},{"location":"others/#Unrestricted-JSZ-model","page":"Other Forms of the Model","title":"Unrestricted JSZ model","text":"","category":"section"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"Our model is the three-factor JSZ(Joslin, Singleton, and Zhu, 2011) model. Under our default option, the JSZ model is constrained by the AFNS(Christensen, Diebold, and Rudebusch, 2011) restriction. Under this restriction, the eigenvalues of the risk-neutral slope matrix are [1, exp(-kappaQ), exp(-kappaQ)].","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"Our package also allows users to estimate three distinct eigenvalues through an option. However, in this case, users must introduce prior distributions for the three eigenvalues. These prior distributions should be the form of Distribution from the Distributions.jl package. For example, the following prior distributions can be introduced:","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"kappaQ_prior_pr = [truncated(Normal(0.9, 0.05), -1, 1), truncated(Normal(0.9, 0.05), -1, 1), truncated(Normal(0.9, 0.05), -1, 1)]","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"The kappaQ_prior_pr containing the prior distributions is a vector of length 3, and each entry is an object from the Distributions.jl package. kappaQ_prior_pr[i] is the prior distribution of the i-th diagonal element of the slope matrix of the VAR system under the risk-neutral measure.","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"After constructing kappaQ_prior_pr, it should be inputted as a keyword argument in functions that require the kappaQ_prior_pr variable (notably tuning_hyperparameter and posterior_sampler). If kappaQ_prior_pr is not provided, the model operates under the AFNS constraint automatically.","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"A point to note when setting prior distributions is that \"the prior expectation of the slope matrix[1:3, 1:3] of the first lag of the VAR model under the physical measure\" is set to diagm(mean.(kappaQ_prior_pr)) under the unrestricted JSZ model. Therefore, the prior distributions of the eigenvalues should be set to reflect the prior expectations under the physical measure to some extent.","category":"page"},{"location":"others/","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"The notation for the three eigenvalues is kappaQ. Therefore, kappaQ is a three-dimensional vector under the unrestricted JSZ model. In contrast, kappaQ is a scalar that represents the DNS decay parameter under the AFNS restriction.","category":"page"},{"location":"estimation/#Estimation","page":"Estimation","title":"Estimation","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"To estimate the model, the following two steps must be undertaken.","category":"page"},{"location":"estimation/#Step-1.-Tuning-Hyperparameters","page":"Estimation","title":"Step 1. Tuning Hyperparameters","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"We have five hyperparameters, p, q, nu0, Omega0, and mean_phi_const.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"p::Float64: lag length of the mathbbP-VAR(p)\nq::Matrix{Float64}( , 4, 2): Shrinkage degrees in the Minnesota prior\nnu0::Float64(d.f.) and Omega0::Vector(diagonals of the scale matrix): Prior distribution of the error covariance matrix in the mathbbP-VAR(p)\nmean_phi_const: Prior mean of the intercept term in the mathbbP-VAR(p)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"We recommend tuning_hyperparameter for deciding the hyperparameters.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"tuned, results = tuning_hyperparameter(yields, macros, tau_n, rho;\n                                        populationsize=50,\n                                        maxiter=10_000,\n                                        medium_tau=collect(24:3:48),\n                                        upper_q=[1 1; 1 1; 10 10; 100 100],\n                                        mean_kQ_infty=0,\n                                        std_kQ_infty=0.1,\n                                        upper_nu0=[],\n                                        mean_phi_const=[],\n                                        fix_const_PC1=false,\n                                        upper_p=18,\n                                        mean_phi_const_PC1=[],\n                                        data_scale=1200,\n                                        kappaQ_prior_pr=[],\n                                        init_nu0=[],\n                                        is_pure_EH=false,\n                                        psi_common=[],\n                                        psi_const=[])","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"Note that the default upper bound of p is upper_p=18. The output tuned::Hyperparameter is the object that needs to be obtained in Step 1. results contains the optimization results.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"If users accept our default values, the function is simplified, that is","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"tuned, results = tuning_hyperparameter(yields, macros, tau_n, rho)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"yields is a T by N matrix, and T is the length of the sample period. N is the number of maturities in data. tau_n is a N-Vector that contains bond maturities in data. For example, if there are two maturities, 3 and 24 months, in the monthly term structure model, tau_n=[3; 24]. macros is a T by dP-dQ matrix in which each column is an individual macroeconomic variable. rho is a dP-dQ-Vector. In general, rho[i] = 1 if macros[:, i] is in level, or it is set to 0 if the macro variable is differenced.","category":"page"},{"location":"estimation/#Several-relevant-points-regarding-hyperparameter-optimization","page":"Estimation","title":"Several relevant points regarding hyperparameter optimization","text":"","category":"section"},{"location":"estimation/#Computational-Cost-of-the-Optimization","page":"Estimation","title":"Computational Cost of the Optimization","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"Since we adopt the Differential Evolutionary(DE) algorithm (Specifically, BlackBoxOptim.jl), it is hard to set the terminal condition. Our strategy was to run the algorithm with a sufficient number of iterations (our default settings) and to verify that it reaches a global optimum by plotting the objective function.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"The reason for using BlackBoxOptim.jl is that this package was the most suitable for our model. After trying several optimization packages in Python and Julia, BlackBoxOptim.jl consistently found the optimum values most reliably. A downside of DE algorithms like BlackBoxOptim.jl is that they can have high computational costs. If the computational cost is excessively high to you, you can reduce it by setting populationsize or maxiter options in tuning_hyperparameter to lower values. However, this may lead to a decrease in model performance.","category":"page"},{"location":"estimation/#Range-of-Data-over-which-the-Marginal-Likelihood-is-Calculated","page":"Estimation","title":"Range of Data over which the Marginal Likelihood is Calculated","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"In Bayesian methodology, the standard criterion of the model comparison is the marginal likelihood. When we compare models using the marginal likelihood, the most crucial prerequisite is that the marginal likelihoods of all models must be calculated over the same observations.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"For instance, let's say we have data with the number of rows being 100. Model 1 has p=1, and Model 2 has p=2. In this case, the marginal likelihood should be computed over data[3:end, :]. This means that for Model 1, data[2, :] is used as the initial value, and for Model 2, data[1:2, :] is used as initial values. tuning_hyperparameter automatically reflects this fact by calculating the marginal likelihood over data[upper_p+1:end, :] for model comparison.","category":"page"},{"location":"estimation/#Prior-Belief-about-the-Expectation-Hypothesis","page":"Estimation","title":"Prior Belief about the Expectation Hypothesis","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"Our algorithm has an inductive bias that the estimates should not deviate too much from the Expectation Hypothesis (EH). Here, the assumed EH means that the term premium is a non-zero constant. If you want to introduce an inductive bias centered around the pure EH, where the term premium is zero, set is_pure_EH=true. However, note that using this option may take some initial time to numerically set the prior distribution.","category":"page"},{"location":"estimation/#Step-2.-Sampling-the-Posterior-Distribution-of-Parameters","page":"Estimation","title":"Step 2. Sampling the Posterior Distribution of Parameters","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"In Step 1, we got tuned::Hyperparameter. posterior_sampler uses it for the estimation.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params, acceptPrMH = posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter;\n                                            medium_tau=collect(24:3:48),\n                                            init_param=[],\n                                            psi=[],\n                                            psi_const=[],\n                                            gamma_bar=[],\n                                            kappaQ_prior_pr=[],\n                                            mean_kQ_infty=0,\n                                            std_kQ_infty=0.1,\n                                            fix_const_PC1=false,\n                                            data_scale=1200)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"If users changed the default values in Step 1, the corresponding default values in the above function also should be changed. If users use our default values, the function simplifies to","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params, acceptPrMH = posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"iteration is the number of posterior samples that users want to get. Our MCMC starts at the prior mean, and you have to erase burn-in samples manually.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params::Vector{Parameter} has a length of iteration and each entry is a posterior sample. acceptPrMH is dQ+1-Vector, and the i(<=dQ)-th entry shows the MH acceptance rate for i-th principal component in the recursive mathbbP-VAR. The last entry of acceptPrMH is the MH acceptance rate for kappaQ under the unrestricted JSZ model. It is zero under the AFNS restriction.","category":"page"},{"location":"estimation/#Step-3.-Discard-Burn-in-and-Nonstationary-Posterior-Samples","page":"Estimation","title":"Step 3. Discard Burn-in and Nonstationary Posterior Samples","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"After users get posterior samples(saved_params), they might want to discard some samples as burn-in. If the number of burn-in samples is burnin, run","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params = saved_params[burnin+1:end]","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"Also, users might want to erase posterior samples that do not satisfies the stationary condition. It can be done by erase_nonstationary_param.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params, Pr_stationary = erase_nonstationary_param(saved_params; threshold=1)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"All entries in the first output (saved_params::Vector{Parameter}) are posterior samples that satisfy the stationary condition.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"warning: Reduction in the Number of Posterior Samples\nThe vector length of saved_params decreases after the burn-in process and erase_nonstationary_param. Note that this leads to a gap between iteration and length(saved_params).","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"note: Handling Non-Stationary Data\nThe optional input eigenvalue is designed to discard posterior samples with eigenvalues of the VAR system exceeding the specified threshold. Traditionally, we use a stationary VAR, so the default threshold is set to 1. However, for non-stationary VAR models, it may be necessary to allow for a slightly higher threshold. In such cases, you can set threshold to a value greater than 1, such as 1.05.","category":"page"},{"location":"estimation/#Diagnostics-for-MCMC","page":"Estimation","title":"Diagnostics for MCMC","text":"","category":"section"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"We believe in the efficiency of our algorithm, so users do not need to be overly concerned about the convergence of the posterior samples. In our opinion, sampling 6,000 posterior samples and erase the first 1,000 samples as burn-in would be enough.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"We provide a measure to gauge the efficiency of the algorithm, that is","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"ineff = ineff_factor(saved_params)","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"saved_params::Vector{Parameter} is the output of posterior_sampler. ineff is Tuple(kappaQ, kQ_infty, gamma, SigmaO, varFF, phi). Each object of the tuple has the same shape as its corresponding parameter. The entries of the Array of the Tuple represent the inefficiency factors of the corresponding parameters. If an inefficiency factor is high, it indicates poor sampling efficiency of the parameter located at the same position.","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"You can calculate the maximum inefficiency factor by","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"max_ineff = (ineff[1] |> maximum, ineff[2], ineff[3] |> maximum, ineff[4] |> maximum, ineff[5] |> maximum, ineff[6] |> maximum) |> maximum","category":"page"},{"location":"estimation/","page":"Estimation","title":"Estimation","text":"The value obtained by dividing the number of posterior samples by max_ineff is the effective number of posterior samples, taking into account the efficiency of the sampler. For example, let's say max_ineff = 10. Then, if 6,000 posterior samples are drawn and the first 1,000 samples are erased as burn-in, the remaining 5,000 posterior samples have the same efficiency as using 500 i.i.d samples, calculated as (6000-1000)/max_ineff. For reference, in our paper, the maximum inefficiency factor was 2.38.","category":"page"},{"location":"notations/#Notations","page":"Notations","title":"Notations","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"Our package is based on our paper. The variable names in our package and the notation in our paper correspond as follows.","category":"page"},{"location":"notations/","page":"Notations","title":"Notations","text":"(Image: Notation Table)","category":"page"},{"location":"#TermStructureModels.jl","page":"Overview","title":"TermStructureModels.jl","text":"","category":"section"},{"location":"","page":"Overview","title":"Overview","text":"TermStructureModels.jl has the below functions.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Statistical Inference\nParameters\nYield Curve Interpolation\nTerm Premiums\nForecasting\nConditional Forecasting without scenarios (Baseline Forecasts)\nScenario Analysis (Scenario Forecasts)","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"To use the above functions, an estimation of the model must first be conducted. That is, use posterior_sampler to obtain posterior samples of parameters. The posterior samples are used for the above functions (Statistical inference and Forecasting). For details, refer to the corresponding pages.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Some outputs of our package are not simple arrays. They are","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Vector{Parameter}: output of posterior_sampler\nVector{ReducedForm}: output of reducedform\nVector{LatentSpace}: output of latentspace\nVector{YieldCurve}: output of fitted_YieldCurve\nVector{TermPremium}: output of term_premium\nVector{Forecast}: outputs of conditional_forecasts and scenario_analysis","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"The above outputs contain information about the posterior distributions of objects of interest. Users can use the outputs above to extract posterior samples or calculate descriptive statistics of the posterior distributions.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Our package is based on our paper. Descriptions of our model and the meanings of each variable can be found in the paper. The Notation section details how notations in the paper correspond to variables in our package. Additionally, the example file used in our paper is available in the repository.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"Users are encouraged to read the two text boxes below.","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"warning: Unit of Data\nTheoretical term structure models typically describe bond yields as decimals per one time period. However, yield data is typically presented in percent per annum. Therefore, you have to address the issue by using the option data_scale. data_scale represents the scale of the data. Specifically,`yields_in_data` = `data_scale`*`theoretical_yields_in_the_model`holds. For example, suppose we have monthly yield data in percent per annum. If we use a monthly term structure model, data_scale=1200. The default value of data_scale is 1200 for all functions.Functions that have option data_scale are as follows:tuning_hyperparameter\nposterior_sampler\nterm_premium\nconditional_forecasts\nscenario_analysis\nlatentspace\nreducedform\nfitted_YieldCurve\ngenerative\ncalibrate_mean_phi_const","category":"page"},{"location":"","page":"Overview","title":"Overview","text":"tip: Normalization of Data\nOur package demeans the principal components of bond yields, which are spanned risk factors in the bond market. Therefore, we recommend using macro data after demeaning it. Of course, demeaning the macro variables is recommended but not mandatory.","category":"page"},{"location":"inference/#Statistical-Inference","page":"Statistical Inference","title":"Statistical Inference","text":"","category":"section"},{"location":"inference/#Inference-for-Parameters","page":"Statistical Inference","title":"Inference for Parameters","text":"","category":"section"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"You can get posterior samples of the term structure model parameters using reducedform.","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"reduced_params = reducedform(saved_params, yields, macros, tau_n; data_scale=1200)","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"yields is a T by N matrix, and T is the length of the sample period. N is the number of bond maturities in data. tau_n is a N-Vector that contains maturities in data. For example, if there are two maturities, 3 and 24 months, in the monthly term structure model, tau_n=[3; 24]. macros is a T by dP-dQ matrix in which each column is an individual macroeconomic variable.","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"note: Reason Why We have to Run `reducedform` in addition to `posterior_sampler`\nWe estimate the mathbbP-VAR by transforming it into a recursive VAR form. Therefore, Parameter, the output of posterior_sampler, contains parameters in the recursive VAR. In contrast, ReducedForm, the output of reducedform, contains parameters in the original reduced-form mathbbP-VAR.","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"Each entry in reduced_params::Vector{ReducedForm} is a joint posterior sample of the parameters.","category":"page"},{"location":"inference/#Yield-Curve-Interpolation","page":"Statistical Inference","title":"Yield Curve Interpolation","text":"","category":"section"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"We first have to transform the parameter space from the principal component space to the latent factor space. It is done by latentspace. And then, use fitted_YieldCurve to get fitted yields. Specifically,","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"saved_latent_params = latentspace(saved_params, yields, tau_n; data_scale=1200)\nfitted_yields = fitted_YieldCurve(τ0, saved_latent_params::Vector{LatentSpace}; data_scale=1200)","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"τ0 is a Vector containing the maturities for which we want to calculate fitted yields through interpolation. fitted_yields::Vector{YieldCurve} contains the results of the interpolation.","category":"page"},{"location":"inference/#Term-Premiums","page":"Statistical Inference","title":"Term Premiums","text":"","category":"section"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"term_premium calculates the term premium of the bonds. tau_maturity contains the maturities of interest, and it should be Vector(at least one-dimensional vector).","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"saved_TP, saved_tv_TP, saved_tv_EH = term_premium(tau_interest, tau_n, saved_params, yields, macros; data_scale=1200)","category":"page"},{"location":"inference/","page":"Statistical Inference","title":"Statistical Inference","text":"saved_TP::Vector{TermPremium} contains the results of the term premium calculations. Both the term premiums and expectation hypothesis components are decomposed into the time-invariant part and time-varying part. For the maturity tau_interest[i], the time-varying parts are saved in saved_tv_TP[:, :, i] and saved_tv_EH[:, :, i]. The time-varying parts driven by the j-th pricing factor is stored in saved_tv_TP[:, j, i] and saved_tv_EH[:, j, i].","category":"page"},{"location":"output/#How-to-Utilize-the-Outputs-of-Functions","page":"Utilization of the Output","title":"How to Utilize the Outputs of Functions","text":"","category":"section"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"When users execute some functions, the output is Vector{<:PosteriorSample}. The examples of Vector{<:PosteriorSample} are","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"Vector{Parameter}: output of posterior_sampler\nVector{ReducedForm}: output of reducedform\nVector{LatentSpace}: output of latentspace\nVector{YieldCurve}: output of fitted_YieldCurve\nVector{TermPremium}: output of term_premium\nVector{Forecast}: outputs of conditional_forecasts and scenario_analysis","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"Each entry of the above vectors is a posterior sample and takes the form of a struct, which is one of the following: Parameter, ReducedForm, LatentSpace, YieldCurve, TermPremium, Forecast. The above six struct have their unique fields. See the API section to see what fields each struct contains. Section Notations explains the specific meanings of the fields.","category":"page"},{"location":"output/#Extract-Posterior-Samples-of-the-fields","page":"Utilization of the Output","title":"Extract Posterior Samples of the fields","text":"","category":"section"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"Vector{<:PosteriorSample} contains posterior samples of the fields of the corresponding struct. You can call posterior samples of a specific field by using getindex. For example, if we want to get posterior samples of phi in Parameter, run","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"samples_phi = saved_params[:phi]","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"for saved_params::Vector{Parameter}. Then, samples_phi is a vector, and samples_phi[i] is the i-th posterior sample of phi. Note that samples_phi[i] is a matrix in this case. (Julialang allows Vector to have Array elements.)","category":"page"},{"location":"output/#Descriptive-Statistics-of-the-Posterior-Distributions","page":"Utilization of the Output","title":"Descriptive Statistics of the Posterior Distributions","text":"","category":"section"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"We extend mean, var, std, median, and quantile from Statistics.jl to Vector{<:PosteriorSample}. These five functions can be conveniently used to calculate descriptive statistics of the posterior distribution, such as the posterior mean or posterior variance. For example, the posterior mean of phi can be calculated by","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"mean_phi = mean(saved_params)[:phi]","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"mean_phi[i,j] is the posterior mean of the entry in the i-th row and j-th column of phi. Outputs of all functions(mean, var, std, median, and quantile) have the same shapes as their corresponding parameters. quantile needs the second input. For example, in the case of","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"q_phi = quantile(saved_params, 0.4)[:phi]","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"40% of posterior samples of phi[i,j] are less than q_phi[i,j].","category":"page"},{"location":"output/","page":"Utilization of the Output","title":"Utilization of the Output","text":"tip: Tip\nTo get posterior samples or posterior descriptive statistics of a specific object, we need to know which struct contains the object as a field. Section Notations organizes which structs contain the object.","category":"page"}]
}
