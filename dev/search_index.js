var documenterSearchIndex = {"docs":
[{"location":"scenario/#Forecasting","page":"Forecasting","title":"Forecasting","text":"The package provides two kinds of forecasts.\n\nBaseline Forecasts\nScenario Forecasts (Scenario Analysis)\n\nBoth forecasts are conditional forecasts, because they are based on information in the data. The difference is that the scenario forecast assumes additional scenarios that describe future paths of some variables.\n\nBaseline forecasts and scenario forecasts can be represented either as the posterior distribution of predicted objects or as the posterior distribution of conditional expectations of predicted objects. To summarize:\n\nPosterior Distribution of Predicted Objects\nIn other words, the distribution of future objects conditional on past observations and the scenario\nFunction: conditional_forecast\nPosterior Distribution of Conditional Expectations of Predicted Objects\nIn other words, the posterior distribution of \"E[future object|past obs, scenario, parameters]\"\nFunction: conditional_expectation\n\nIn this summary, for baseline forecasts, the scenario is the empty set.\n\nThe first one is the full Bayesian treatment, so it is mathematically strict. However, it can be difficult to derive meaningful implications from the prediction because of its wide prediction intervals. The second one considers only parameter uncertainty, so it underestimates the prediction uncertainty. However, it is appropriate when you make decisions based on the expected path of future variables. We recommend the second version (conditional_expectation).\n\nThe required inputs and the type of the output are the same between conditional_forecast and conditional_expectation. That is,\n\nprojections = conditional_forecast(S::Vector, tau, horizon, saved_params, yields, macros, tau_n; baseline=[], mean_macros::Vector=[], data_scale=1200, pca_loadings=[])\n\nand\n\nprojections = conditional_expectation(S::Vector, tau, horizon, saved_params, yields, macros, tau_n; baseline=[], mean_macros::Vector=[], data_scale=1200, pca_loadings=[])\n\nprojections::Vector{Forecast} contains the results of the forecasting. tau is a vector, and the term premium of tau[i]-bond is forecasted for each i. If tau is set to [], the term premium is not forecasted. horizon is the forecasting horizon. horizon should not be smaller than length(S). saved_params::Vector{Parameter} is the output of posterior_sampler.\n\nYou can use the same yields, tau_n and macros you employed when executing posterior_sampler. If you wish to compute conditional forecasts using observations up to a certain point, you can simply use yields and macros from the initial period up to that point. However, parameter uncertainty is incorporated independently of yields and macros through saved_params.\n\nIf you use demeaned macro data, option mean_macros is useful. If the sample mean of macro data is specified as the input value for mean_macros, projections contains conditional forecasts of non-demeaned macro variables. The sample mean of macro data can be calculated as follows.\n\nmean_macros = mean(raw_macros_data, dims=1)[1, :]\n\nwarning: Option `mean_macros`\nIf macro variables are not demeaned, ignore option mean_macros.\n\nS determines whether we are computing a baseline forecast or a scenario forecast. How S is set will be described in the following sections.","category":"section"},{"location":"scenario/#Baseline-Forecasts","page":"Forecasting","title":"Baseline Forecasts","text":"Do\n\nS = []\n\nIt sets a scenario to an empty set, so the package calculates baseline forecasts.","category":"section"},{"location":"scenario/#Scenario-Forecasts","page":"Forecasting","title":"Scenario Forecasts","text":"S should be Vector{Scenario}. S can be initialized by\n\nS = Vector{Scenario}(undef, len)\n\nlen is the length of S. For example, if the scenario is assumed for the next 5 time periods, len=5.\n\nS[i] represents the scenario for future variables at time T+i, where T refers to the time of the last observation in macros and yields. The type of S[i] is Scenario, and struct Scenario has two fields: combinations::Matrix and values::Vector. The fields in S[i] are implicitly defined by\n\nS[i].combination*[yields[T+i,:]; macros[T+i, :]] == S[i].values\n\n[yields[T+i,:]; macros[T+i, :]] is a predicted variable that is not observed. Scenario forecasts are calculated assuming that the above equation holds at time T+i, based on S[i] set by you. The number of rows in S[i].combination and the length of S[i].values are the same, and this length represents the number of scenarios assumed at time T+i.\n\nSetting the two fields of S[i] is straightforward. Suppose that the content of the scenarios at time T+i is\n\ncombs*[yields[T+i,:]; macros[T+i, :]] == vals\n\nThen, you can assign the content to S[i] by executing\n\nS[i] = Scenario(combinations=combs, values=vals)","category":"section"},{"location":"api/#API-documentation","page":"API","title":"API documentation","text":"Pages = [\"api.md\"]","category":"section"},{"location":"api/#Index","page":"API","title":"Index","text":"Pages = [\"api.md\"]","category":"section"},{"location":"api/#Exported-Functions","page":"API","title":"Exported Functions","text":"","category":"section"},{"location":"api/#Internal-Functions","page":"API","title":"Internal Functions","text":"","category":"section"},{"location":"api/#TermStructureModels.Forecast","page":"API","title":"TermStructureModels.Forecast","text":"@kwdef struct Forecast <: PosteriorSample\n\nThis struct contains the results of the scenario analysis, the conditional prediction for yields, factors = [PCs macros], and term premiums.\n\nyields\nfactors\nTP: term premium forecasts\nEH: estimated expectation hypothesis component\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Hyperparameter","page":"API","title":"TermStructureModels.Hyperparameter","text":"@kwdef struct Hyperparameter\n\np::Int\nq::Matrix\nnu0\nOmega0::Vector\nmean_phi_const::Vector = zeros(length(Omega0)): This is the prior mean of the constant term in the VAR.\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.LatentSpace","page":"API","title":"TermStructureModels.LatentSpace","text":"@kwdef struct LatentSpace <: PosteriorSample\n\nWhen the model goes to the JSZ latent factor space, the statistical parameters in struct Parameter are also transformed. This struct contains the transformed parameters. Specifically, the transformation is latents[t,:] = T0P_ + inv(T1X)*PCs[t,:]. \n\nIn the latent factor space, the transition equation is data[t,:] = KPXF + GPXFXF*vec(data[t-1:-1:t-p,:]') + MvNormal(O,OmegaXFXF), where data = [latent macros].\n\nlatents::Matrix\nkappaQ\nkQ_infty\nKPXF::Vector\nGPXFXF::Matrix\nOmegaXFXF::Matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Parameter","page":"API","title":"TermStructureModels.Parameter","text":"@kwdef struct Parameter <: PosteriorSample\n\nThis struct contains the statistical parameters of the model that are sampled from function posterior_sampler.\n\nkappaQ\nkQ_infty::Float64\nphi::Matrix{Float64}\nvarFF::Vector{Float64}\nSigmaO::Vector{Float64}\ngamma::Vector{Float64}\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Parameter_NUTS","page":"API","title":"TermStructureModels.Parameter_NUTS","text":"@kwdef struct Parameter_NUTS <: PosteriorSample\n\nThis struct contains the statistical parameters of the model that are sampled from function posterior_NUTS.\n\nq\nnu0\nkappaQ\nkQ_infty::Float64\nphi::Matrix{Float64}\nvarFF::Vector{Float64}\nSigmaO::Vector{Float64}\ngamma::Vector{Float64}\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.PosteriorSample","page":"API","title":"TermStructureModels.PosteriorSample","text":"abstract type PosteriorSample\n\nThis is a super-set of structs Parameter, ReducedForm, LatentSpace, YieldCurve, TermPremium, Forecast.\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.ReducedForm","page":"API","title":"TermStructureModels.ReducedForm","text":"@kwdef struct ReducedForm <: PosteriorSample\n\nThis struct contains the statistical parameters in terms of the reduced form VAR(p) in P-dynamics. lambdaP and LambdaPF are parameters in the market prices of risks equation, and they only contain the first dQ non-zero equations. \n\nkappaQ\nkQ_infty\nKPF\nGPFF\nOmegaFF::Matrix\nSigmaO::Vector\nlambdaP\nLambdaPF\nmpr::Matrix(market prices of risks, T, dP)\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.Scenario","page":"API","title":"TermStructureModels.Scenario","text":"@kwdef struct Scenario\n\nThis struct contains scenarios to be conditioned in the scenario analysis. When y = [yields; macros] is an observed vector in the measurement equation, Scenario.combinations*y = Scenario.values constitutes the scenario at a specific time. Vector{Scenario} is used to describe a time-series of scenarios.\n\ncombinations and values should be Matrix and Vector. If values is a scalar, combinations would be a matrix with one row vector and values should be one-dimensional vector, for example [values]. \n\ncombinations::Matrix\nvalues::Vector\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.TermPremium","page":"API","title":"TermStructureModels.TermPremium","text":"@kwdef struct TermPremium <: PosteriorSample\n\nThe yields are decomposed into the term premium(TP) and the expectation hypothesis component(EH). Each component has constant terms(const_TP and const_EH) and time-varying components(timevarying_TP and timevarying_EH). factorloading_EH and factorloading_TP are coefficients of the pricing factors for the time varying components. Each column of the outputs indicates the results for each maturity.\n\nThe time-varying components are not stored in TermPremium, and they are the separate outputs in function term_premium. \n\nTP\nEH\nfactorloading_TP\nfactorloading_EH\nconst_TP\nconst_EH\n\n\n\n\n\n","category":"type"},{"location":"api/#TermStructureModels.YieldCurve","page":"API","title":"TermStructureModels.YieldCurve","text":"@kwdef struct YieldCurve <: PosteriorSample\n\nThis struct contains the fitted yield curve. yields[t,:] = intercept + slope*latents[t,:] holds.\n\nlatents::Matrix: latent pricing factors in LatentSpace\nyields\nintercept\nslope\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.getindex-Tuple{PosteriorSample, Symbol}","page":"API","title":"Base.getindex","text":"getindex(x::PosteriorSample, c::Symbol)\n\nFor struct <: PosteriorSample, struct[:name] calls objects in struct.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.getindex-Tuple{Vector{<:PosteriorSample}, Symbol}","page":"API","title":"Base.getindex","text":"getindex(x::Vector{<:PosteriorSample}, c::Symbol)\n\nFor struct <: PosteriorSample, struct[:name] calls objects in struct. Output[i] = the i-th posterior sample\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{Vector{<:PosteriorSample}}","page":"API","title":"Statistics.mean","text":"mean(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior mean.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.median-Tuple{Vector{<:PosteriorSample}}","page":"API","title":"Statistics.median","text":"median(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior median.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.quantile-Tuple{Vector{<:PosteriorSample}, Any}","page":"API","title":"Statistics.quantile","text":"quantile(x::Vector{<:PosteriorSample}, q)\n\nOutput[:variable name] returns a quantile of the corresponding posterior distribution.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.std-Tuple{Vector{<:PosteriorSample}}","page":"API","title":"Statistics.std","text":"std(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior standard deviation.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{Vector{<:PosteriorSample}}","page":"API","title":"Statistics.var","text":"var(x::Vector{<:PosteriorSample})\n\nOutput[:variable name] returns the corresponding posterior variance.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.AR_res_var-Tuple{Vector, Any}","page":"API","title":"TermStructureModels.AR_res_var","text":"AR_res_var(TS::Vector, p)\n\nThis function derives the MLE error variance estimate of an AR(p) model.\n\nInput\n\nUnivariate time series TS and lag p\n\nOutput(2)\n\nResidual variance estimate, AR(p) coefficients\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.GQ_XX-Tuple{}","page":"API","title":"TermStructureModels.GQ_XX","text":"GQ_XX(; kappaQ)\n\nkappaQ governs the conditional mean of the Q-dynamics of X, and its slope matrix has a restricted form. This function shows that restricted form.\n\nOutput\n\nslope matrix of the Q-conditional mean of X\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.LDL-Tuple{Any}","page":"API","title":"TermStructureModels.LDL","text":"LDL(X)\n\nThis function generates a matrix decomposition called LDLt. X = L*D*L', where L is a lower triangular matrix and D is a diagonal. How to conduct it can be found at Wikipedia.\n\nInput\n\nDecomposed Object, X\n\nOutput(2)\n\nL, D\n\nDecomposed result is X = L*D*L'\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.PCA-Tuple{Any, Any}","page":"API","title":"TermStructureModels.PCA","text":"PCA(yields, p; pca_loadings=[], dQ=[])\n\nThis function derives the principal components from yields.\n\nInput\n\nyields[p+1:end, :] is used to construct the affine transformation, and then all yields[:,:] are transformed into the principal components.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\n\nOutput(4)\n\nPCs, OCs, Wₚ, Wₒ, mean_PCs\n\nPCs, OCs: first dQ and the remaining principal components\nWₚ, Wₒ: the rotation matrix for PCs and OCs, respectively\nmean_PCs: the mean of PCs before being demeaned.\nPCs are demeaned.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.calibrate_mean_phi_const-NTuple{7, Any}","page":"API","title":"TermStructureModels.calibrate_mean_phi_const","text":"calibrate_mean_phi_const(mean_kQ_infty, std_kQ_infty, nu0, yields, macros, tau_n, p; mean_phi_const_PCs=[], medium_tau=collect(24:3:48), iteration=1000, data_scale=1200, kappaQ_prior_pr=[], τ=[], pca_loadings=[])\n\nThis function calibrates a prior mean of the first dQ constant terms in the VAR. Adjust your prior setting based on the prior samples in the outputs.\n\nInput\n\nmean_phi_const_PCs is your prior mean of the first dQ constants. The default option sets it as a zero vector.\niteration is the number of prior samples.\nτ::scalar is a maturity for calculating the constant part in the term premium.\nIf τ is empty, the function does not sample the prior distribution of the constant part in the term premium.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\n\nOutput(2)\n\nprior_λₚ, prior_TP\n\nsamples from the prior distribution of λₚ\nprior samples of the constant part in the τ-month term premium\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.conditional_expectation-Tuple{Vector, Vararg{Any, 6}}","page":"API","title":"TermStructureModels.conditional_expectation","text":"conditional_expectation(S::Vector, tau, horizon, saved_params, yields, macros, tau_n; baseline=[], mean_macros::Vector=[], data_scale=1200, pca_loadings=[], is_parallel=false)\n\nInput\n\nscenarios, a result of the posterior sampler, and data \n\nS[t] = conditioned scenario at time size(yields, 1)+t.\nSet S = [] if you need an unconditional prediction.\nIf you are conditioning a scenario, I assume S = Vector{Scenario}.\ntau is a vector of maturities that term premiums of interest has.\nhorizon: maximum length of the predicted path. It should not be smaller than length(S).\nsaved_params: the first output of function posterior_sampler.\nbaseline::Vector{Forecast}: baseline is the output of conditional_expectation. It is generally set as the result when S is empty. When provided, conditional forecasts represent deviations from baseline.\nmean_macros::Vector: If you demeaned macro variables, you can input the mean of the macro variables. Then, the output will be generated in terms of the un-demeaned macro variables.\nIf mean_macros was used as an input when deriving baseline with this function, mean_macros should also be included as an input when using baseline as an input. Conversely, if mean_macros was not used as an input when deriving baseline, it should not be included as an input when using baseline.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nVector{Forecast}(, iteration)\nt-th rows in predicted yields, predicted factors, predicted TP, and predicted EH are the corresponding predicted value at time size(yields, 1)+t.\nMathematically, it is a posterior distribution of E[future obs|past obs, scenario, parameters].\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.conditional_forecast-Tuple{Vector, Vararg{Any, 6}}","page":"API","title":"TermStructureModels.conditional_forecast","text":"conditional_forecast(S::Vector, tau, horizon, saved_params, yields, macros, tau_n; baseline=[], mean_macros::Vector=[], data_scale=1200, pca_loadings=[], is_parallel=false)\n\nInput\n\nscenarios, a result of the posterior sampler, and data \n\nS[t] = conditioned scenario at time size(yields, 1)+t.\nIf we need an unconditional prediction, S = [].\nIf you are conditioning a scenario, I assume S = Vector{Scenario}.\ntau is a vector. The term premium of tau[i]-bond is forecasted for each i.\nIf tau is set to [], the term premium is not forecasted.\nhorizon: maximum length of the predicted path. It should not be smaller than length(S).\nsaved_params: the first output of function posterior_sampler.\nbaseline::Vector{Forecast}: baseline is the output of conditional_forecast. It is generally set as the result when S is empty. When provided, conditional forecasts represent deviations from baseline.\nmean_macros::Vector: If you demeaned macro variables, you can input the mean of the macro variables. Then, the output will be generated in terms of the un-demeaned macro variables.\nIf mean_macros was used as an input when deriving baseline with this function, mean_macros should also be included as an input when using baseline as an input. Conversely, if mean_macros was not used as an input when deriving baseline, it should not be included as an input when using baseline.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nVector{Forecast}(, iteration)\nt-th rows in predicted yields, predicted factors, predicted TP, and predicted EH are the corresponding predicted value at time size(yields, 1)+t.\nMathematically, it is a posterior sample from future observation|past observation,scenario.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.dcurvature_dτ-Tuple{Any}","page":"API","title":"TermStructureModels.dcurvature_dτ","text":"dcurvature_dτ(τ; kappaQ)\n\nThis function calculates the first derivative of the curvature factor loading w.r.t. the maturity.\n\nInput\n\nkappaQ: The decay parameter\nτ: The maturity at which the derivative is calculated\n\nOutput\n\nthe first derivative of the curvature factor loading w.r.t. the maturity\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.dimQ-Tuple{}","page":"API","title":"TermStructureModels.dimQ","text":"dimQ()\n\nThis function returns the dimension of Q-dynamics under the standard ATSM.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.erase_nonstationary_param-Tuple{Vector{Parameter_NUTS}}","page":"API","title":"TermStructureModels.erase_nonstationary_param","text":"erase_nonstationary_param(saved_params::Vector{Parameter_NUTS}; threshold=1)\n\nThis function filters out posterior samples that imply a unit root VAR system. Only stationary posterior samples remain.\n\nInput\n\nsaved_params is the output of function posterior_NUTS.\nPosterior samples with eigenvalues of the P-system greater than threshold are removed.\n\nOutput(2):\n\nstationary samples, acceptance rate(%)\n\nThe second output indicates how many posterior samples remain.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.erase_nonstationary_param-Tuple{Vector{Parameter}}","page":"API","title":"TermStructureModels.erase_nonstationary_param","text":"erase_nonstationary_param(saved_params::Vector{Parameter}; threshold=1)\n\nThis function filters out posterior samples that imply a unit root VAR system. Only stationary posterior samples remain.\n\nInput\n\nsaved_params is the first output of function posterior_sampler.\nPosterior samples with eigenvalues of the P-system greater than threshold are removed.\n\nOutput(2):\n\nstationary samples, acceptance rate(%)\n\nThe second output indicates how many posterior samples remain.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.fitted_yieldcurve-Tuple{Any, Vector{LatentSpace}}","page":"API","title":"TermStructureModels.fitted_yieldcurve","text":"fitted_yieldcurve(tau_vec, saved_latent_params::Vector{LatentSpace}; data_scale=1200, is_parallel=false)\n\nThis function generates the fitted yield curve.\n\nInput\n\ntau_vec is a set of maturities of interest. tau_vec does not need to be the same as the one used for the estimation.\nsaved_latent_params is a transformed posterior sample using function latentspace.\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nVector{YieldCurve}(,# of iteration)\nyields and latents contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.generative-Tuple{Any, Any, Any, Any, Float64}","page":"API","title":"TermStructureModels.generative","text":"generative(T, dP, tau_n, p, noise::Float64; kappaQ, kQ_infty, KPXF, GPXFXF, OmegaXFXF, data_scale=1200)\n\nThis function generates simulation data given parameters. Note that all parameters are in the latent factor state space (i.e., parameters in struct LatentSpace). There are some differences in notation because it is difficult to express mathcal letters in VSCode. Therefore, mathcal{F} in the paper is expressed as F in VSCode, and \"F\" in the paper is expressed as XF.\n\nInput\n\nnoise: Variance of the measurement errors\n\nOutput(3)\n\nyields, latents, macros\n\nyields = Matrix{Float64}(obs,T,length(tau_n))\nlatents = Matrix{Float64}(obs,T,dimQ())\nmacros = Matrix{Float64}(obs,T,dP - dimQ())\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.hessian","page":"API","title":"TermStructureModels.hessian","text":"hessian(f, x, index=[])\n\nThis function calculates the Hessian matrix of a scalar function f at x. If index is not empty, it calculates the Hessian matrix of the function with respect to the selected variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#TermStructureModels.ineff_factor-Tuple{Vector{Parameter_NUTS}}","page":"API","title":"TermStructureModels.ineff_factor","text":"ineff_factor(saved_params::Vector{Parameter_NUTS}; is_parallel=false)\n\nThis function returns the inefficiency factors for each parameter.\n\nInput\n\nVector{Parameter_NUTS} from posterior_NUTS\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nEstimated inefficiency factors are returned as a Tuple(q, nu0, kappaQ, kQ_infty, gamma, SigmaO, varFF, phi). For example, if you want to access the inefficiency factor of phi, you can use Output.phi.\nIf fix_const_PC1==true in your optimized Hyperparameter struct, Output.phi[1,1] may be unreliable and should be ignored.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.ineff_factor-Tuple{Vector{Parameter}}","page":"API","title":"TermStructureModels.ineff_factor","text":"ineff_factor(saved_params::Vector{Parameter}; is_parallel=false)\n\nThis function returns the inefficiency factors for each parameter.\n\nInput\n\nVector{Parameter} from posterior_sampler\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nEstimated inefficiency factors are returned as a Tuple(kappaQ, kQ_infty, gamma, SigmaO, varFF, phi). For example, if you want to access the inefficiency factor of phi, you can use Output.phi.\nIf fix_const_PC1==true in your optimized Hyperparameter struct, Output.phi[1,1] may be unreliable and should be ignored.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.isstationary-Tuple{Any}","page":"API","title":"TermStructureModels.isstationary","text":"isstationary(GPFF; threshold)\n\nThis function checks whether a reduced VAR matrix has unit roots. If there is at least one unit root, the return is false.\n\nInput\n\nGPFF should not include intercepts. Also, GPFF is a dP by dP*p matrix where the coefficient at lag 1 comes first, and the lag p slope matrix comes last.\nPosterior samples with eigenvalues of the P-system greater than threshold are removed. Typically, threshold is set to 1.\n\nOutput\n\nboolean\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.latentspace-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.latentspace","text":"latentspace(saved_params, yields, tau_n; data_scale=1200, pca_loadings=[], is_parallel=false)\n\nThis function translates the principal components state space into the latent factor state space.\n\nInput\n\ndata_scale::scalar: In typical affine term structure models, theoretical yields are in decimal and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nVector{LatentSpace}(, iteration)\nLatent factors contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.log_marginal-Tuple{Any, Any, Any, Hyperparameter, Any, Any}","page":"API","title":"TermStructureModels.log_marginal","text":"log_marginal(PCs, macros, rho, tuned::Hyperparameter, tau_n, Wₚ; psi=[], psi_const=[], medium_tau, kappaQ_prior_pr, fix_const_PC1)\n\nThis file calculates a value of the marginal likelihood. Only the transition equation is used to calculate it.\n\nInput\n\ntuned is a point where the marginal likelihood is evaluated.\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. An empty default value means that you do not use this function. [psi_const psi][i,j] corresponds to phi[:,1:1+dP*p][i,j].\n\nOutput\n\nthe log marginal likelihood of the VAR system.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_mea-Tuple{Any, Any}","page":"API","title":"TermStructureModels.loglik_mea","text":"loglik_mea(yields, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale, pca_loadings)\n\nThis function generates the log likelihood of the measurement equation.\n\nOutput\n\nthe measurement equation part of the log likelihood\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_tran-Tuple{Any, Any}","page":"API","title":"TermStructureModels.loglik_tran","text":"loglik_tran(PCs, macros; phi, varFF)\n\nThis function calculates the log likelihood of the transition equation.\n\nOutput\n\nlog likelihood of the transition equation.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_2_phi₀_C-Tuple{}","page":"API","title":"TermStructureModels.phi_2_phi₀_C","text":"phi_2_phi₀_C(; phi)\n\nThis function divides phi into the lagged regressor part and the contemporaneous regressor part.\n\nOutput(3)\n\nphi0, C = C0 + I, C0\n\nphi0: coefficients for the lagged regressors\nC: coefficients for the dependent variables when all contemporaneous variables are on the LHS of the orthogonalized equations. Therefore, the diagonals of C are ones. Note that since the contemporaneous variables get negative signs when they are on the RHS, the signs of C do not change whether they are on the RHS or LHS.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_varFF_2_OmegaFF-Tuple{}","page":"API","title":"TermStructureModels.phi_varFF_2_OmegaFF","text":"phi_varFF_2_OmegaFF(; phi, varFF)\n\nThis function constructs OmegaFF from statistical parameters.\n\nOutput\n\nOmegaFF\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.posterior_NUTS-NTuple{7, Any}","page":"API","title":"TermStructureModels.posterior_NUTS","text":"posterior_NUTS(p, yields, macros, tau_n, rho, NUTS_nadapt, iteration; init_param=[], prior_q, prior_nu0, psi=[], psi_const=[], gamma_bar=[], prior_mean_diff_kappaQ, prior_std_diff_kappaQ, mean_kQ_infty=0, std_kQ_infty=0.1, fix_const_PC1=false, data_scale=1200, pca_loadings=[], NUTS_target_acceptance_rate=0.65, NUTS_max_depth=10)\n\nThis function implements the NUTS-within-Gibbs sampler. Gibbs blocks that cannot be updated with conjugate priors are sampled using the NUTS sampler.\n\nInput\n\np: The lag length of the VAR system\nNUTS_nadapt: Number of iterations for tuning settings in the NUTS sampler. The warmup samples are included in the output, so you should discard them.\niteration: Number of posterior samples\ninit_param: Starting point of the sampler. It should be of type Parameter_NUTS.\nprior_q: A 4 by 2 matrix that contains the prior distribution for q. All entries should be objects in Distributions.jl. For hyperparameters that do not need to be optimized, assigning a Dirac(::Float64) prior to the corresponding entry fixes that hyperparameter and optimizes only the remaining hyperparameters.\nprior_nu0: The prior distribution for nu0 - (dP + 1). It should be an object in Distributions.jl.\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. An empty default value means that you do not use this function. [psi_const psi][i,j] corresponds to phi[:,1:1+dP*p][i,j]. psi should be a (dP × dP*p) matrix.\nprior_mean_diff_kappaQ and prior_std_diff_kappaQ are vectors that contain the means and standard deviations of the Normal distributions for [kappaQ[1]; diff(kappaQ)]. Once Normal priors are assigned to these parameters, the prior for kappaQ[1] is truncated to (0, 1), and the priors for diff(kappaQ) are truncated to (−1, 0).\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\nNUTS_target_acceptance_rate, NUTS_max_depth are the arguments of the NUTS sampler in AdvancedHMC.jl.\n\nOutput\n\nVector{Parameter_NUTS}(posterior, iteration)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.posterior_sampler-Tuple{Any, Any, Any, Any, Any, Hyperparameter}","page":"API","title":"TermStructureModels.posterior_sampler","text":"posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter; medium_tau=collect(24:3:48), init_param=[], psi=[], psi_const=[], gamma_bar=[], kappaQ_prior_pr=[], mean_kQ_infty=0, std_kQ_infty=0.1, fix_const_PC1=false, data_scale=1200, pca_loadings=[], kappaQ_proposal_mode=[])\n\nThis function samples from the posterior distribution.\n\nInput\n\niteration: Number of posterior samples\ntuned: Optimized hyperparameters used during estimation\ninit_param: Starting point of the sampler. It should be of type Parameter.\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. An empty default value means that you do not use this function. [psi_const psi][i,j] corresponds to phi[:,1:1+dP*p][i,j]. psi should be a (dP × dP*p) matrix.\nkappaQ_prior_pr is a vector of prior distributions for kappaQ under the JSZ model: each element specifies the prior for kappaQ[i] and must be provided as a Distributions.jl object. This option is only needed when using the JSZ model.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\nkappaQ_proposal_mode=Vector{, dQ} contains the center of the proposal distribution for kappaQ. If it is empty, it is optimized by MLE.\n\nOutput(2)\n\nVector{Parameter}(posterior, iteration), acceptance rate of the MH algorithm\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_kappaQ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.prior_kappaQ","text":"prior_kappaQ(medium_tau, pr)\n\nThis function derives the maximizer decay parameter kappaQ that maximizes the curvature factor loading at each candidate medium-term maturity. Then, it imposes a discrete prior distribution on the maximizers with a prior probability vector pr.\n\nInput\n\nmedium_tau::Vector(candidate medium maturities, # of candidates)\npr::Vector(probability, # of candidates)\n\nOutput\n\ndiscrete prior distribution that has a support of the maximizers kappaQ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.reducedform-NTuple{4, Any}","page":"API","title":"TermStructureModels.reducedform","text":"reducedform(saved_params, yields, macros, tau_n; data_scale=1200, pca_loadings=[], is_parallel=false)\n\nThis function converts posterior samples to the reduced form VAR parameters.\n\nInput\n\nsaved_params is the first output of function posterior_sampler.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput\n\nPosterior samples in terms of struct ReducedForm\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.term_premium-NTuple{5, Any}","page":"API","title":"TermStructureModels.term_premium","text":"term_premium(tau_interest, tau_n, saved_params, yields, macros; data_scale=1200, pca_loadings=[], is_parallel=false)\n\nThis function generates posterior samples of the term premiums.\n\nInput\n\nMaturity of interest tau_interest for calculating TP\nsaved_params from function posterior_sampler\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\nis_parallel enables multi-threaded parallel computation when set to true.\n\nOutput(3)\n\nsaved_TP, saved_tv_TP, saved_tv_EH\n\nsaved_TP::Vector{TermPremium}(, iteration)\nsaved_tv_TP::Vector{Array}(, iteration)\nsaved_tv_EH::Vector{Array}(, iteration)\nBoth the term premiums and expectation hypothesis components are decomposed into the time-invariant part and time-varying part. For the maturity tau_interest[i] and j-th posterior sample, the time-varying parts are saved in saved_tv_TP[j][:, :, i] and saved_tv_EH[j][:, :, i]. The time-varying parts driven by the k-th pricing factor are stored in saved_tv_TP[j][:, k, i] and saved_tv_EH[j][:, k, i].\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.tuning_hyperparameter-NTuple{4, Any}","page":"API","title":"TermStructureModels.tuning_hyperparameter","text":"tuning_hyperparameter(yields, macros, tau_n, rho; populationsize=50, maxiter=10_000, medium_tau=collect(24:3:48), upper_q=[1 1; 1 1; 1 1; 4 4; 100 100], mean_kQ_infty=0, std_kQ_infty=0.1, upper_nu0=[], mean_phi_const=[], fix_const_PC1=false, upper_p=24, mean_phi_const_PC1=[], data_scale=1200, kappaQ_prior_pr=[], init_nu0=[], is_pure_EH=false, psi=[], psi_const=[], pca_loadings=[], prior_mean_diff_kappaQ=[], prior_std_diff_kappaQ=[], optimizer=:LBFGS, ml_tol=1.0, init_x=[])\n\nThis function optimizes the hyperparameters by maximizing the marginal likelihood of the transition equation.\n\nInput\n\nWhen comparing marginal likelihoods between models, the data for the dependent variable should be the same across models. To achieve this, we set the period of the dependent variable based on upper_p. For example, if upper_p = 3, yields[4:end,:] and macros[4:end,:] are the data for the dependent variable. yields[1:3,:] and macros[1:3,:] are used for setting initial observations for all lags.\noptimizer: The optimization algorithm to use.\n:LBFGS (default): Uses unconstrained LBFGS from Optim.jl with hybrid parameter transformations (exp for non-negativity, sigmoid for bounded parameters). Alternates between optimizing hyperparameters (with fixed lag) and selecting the best lag (with fixed hyperparameters) until convergence.\n:BBO: Uses a differential evolutionary algorithm (BlackBoxOptim.jl). The lag and hyperparameters are optimized simultaneously.\nml_tol: Tolerance for parsimony in lag selection (only for :LBFGS). After finding the lag with the best marginal likelihood, the algorithm iteratively selects smaller lags if their marginal likelihood is within ml_tol of the best. This favors simpler models (smaller lags) when performance is comparable.\ninit_x: Initial values for hyperparameters and lag (only for :LBFGS). Should be a vector of length 12 in the format [vec(q); nu0-(dP+1); p]. If empty (default), uses [0.1, 0.1, 0.1, 2.0, 1.0, 0.1, 0.1, 0.1, 2.0, 1.0, 1.0, 1].\npopulationsize and maxiter are options for the optimizer.\npopulationsize: the number of candidate solutions in each generation (only for :BBO)\nmaxiter: the maximum number of iterations\nThe lower bounds for q and nu0 are 0 and dP+2.\nThe upper bounds for q, nu0, and VAR lag can be set by upper_q, upper_nu0, and upper_p.\nThe default option for upper_nu0 is the time-series length of the data.\nIf you use the default option for mean_phi_const,\nmean_phi_const[dQ+1:end] is a zero vector.\nmean_phi_const[1:dQ] is calibrated to make the prior mean of λₚ a zero vector.\nAfter step 2, mean_phi_const[1] is replaced with mean_phi_const_PC1 if it is not empty.\nmean_phi_const = Matrix(your prior, dP, upper_p)\nmean_phi_const[:,i] is the prior mean for the VAR(i) constant. Therefore, mean_phi_const is a matrix only in this function. In other functions, mean_phi_const is a vector for the orthogonalized VAR system with the selected lag.\nWhen fix_const_PC1==true, the first element in the constant term in the orthogonalized VAR is fixed to its prior mean during posterior sampling.\ndata_scale::scalar: In a typical affine term structure model, theoretical yields are in decimals and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields and use (data_scale*theoretical yields) as the variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\nkappaQ_prior_pr is a vector of prior distributions for kappaQ under the JSZ model: each element specifies the prior for kappaQ[i] and must be provided as a Distributions.jl object. Alternatively, you can supply prior_mean_diff_kappaQ and prior_std_diff_kappaQ, which define means and standard deviations for Normal priors on [kappaQ[1]; diff(kappaQ)]; the implied Normal prior for each kappaQ[i] is then truncated to (0, 1). These options are only needed when using the JSZ model.\nis_pure_EH::Bool: When mean_phi_const=[], is_pure_EH=false sets mean_phi_const to zero vectors. Otherwise, mean_phi_const is set to imply the pure expectation hypothesis under mean_phi_const=[].\npsi_const and psi are multiplied with prior variances of coefficients of the intercept and lagged regressors in the orthogonalized transition equation. They are used for imposing zero prior variances. An empty default value means that you do not use this function. [psi_const psi][i,j] corresponds to phi[:,1:1+dP*p][i,j]. psi is a (dP × dPupperp) matrix; when a shorter lag p < upperp is selected, `psi[:, 1:dPp]` is automatically used.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\n\nOutput(2)\n\nOptimized hyperparameter, optimization result\n\nNote that we minimize the negative log marginal likelihood, so the second output is for the minimization problem.\nWhen optimizer=:LBFGS, the second output is a NamedTuple with fields minimizer, minimum, p, all_minimizer, all_minimum.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.tuning_hyperparameter_with_vs-NTuple{4, Any}","page":"API","title":"TermStructureModels.tuning_hyperparameter_with_vs","text":"tuning_hyperparameter_with_vs(yields, macros, tau_n, rho; populationsize=50, maxiter=10_000, medium_tau=collect(24:3:48), upper_q=[1 1; 1 1; 1 1; 4 4; 100 100], mean_kQ_infty=0, std_kQ_infty=0.1, upper_nu0=[], mean_phi_const=[], fix_const_PC1=false, upper_p=24, mean_phi_const_PC1=[], data_scale=1200, kappaQ_prior_pr=[], init_nu0=[], is_pure_EH=false, psi_const=[], pca_loadings=[], prior_mean_diff_kappaQ=[], prior_std_diff_kappaQ=[], optimizer=:LBFGS, ml_tol=1.0, init_x=[])\n\nThis function optimizes the hyperparameters with automatic variable selection: selects which macro variables affect latent factors (PCs).\n\nInput\n\nWhen comparing marginal likelihoods between models, the data for the dependent variable should be the same across models. To achieve this, we set the period of the dependent variable based on upper_p. For example, if upper_p = 3, yields[4:end,:] and macros[4:end,:] are the data for the dependent variable. yields[1:3,:] and macros[1:3,:] are used for setting initial observations for all lags.\noptimizer: The optimization algorithm to use.\n:LBFGS (default): Alternates between lag selection, forward stepwise variable selection for coefficients of macro variables on latent factors, and hyperparameter optimization. Variable selection stops when log marginal likelihood improvement ≤ 1.0.\n:BBO: Uses BlackBoxOptim.jl to optimize lag, hyperparameters, and variable selection simultaneously.\nml_tol: Tolerance for parsimony in lag selection (only for :LBFGS). After finding the lag with the best marginal likelihood, the algorithm iteratively selects smaller lags if their marginal likelihood is within ml_tol of the best. This favors simpler models (smaller lags) when performance is comparable.\ninit_x: Initial values for hyperparameters and lag (only for :LBFGS). Should be a vector of length 12 in the format [vec(q); nu0-(dP+1); p]. If empty (default), uses [0.1, 0.1, 0.1, 2.0, 1.0, 0.1, 0.1, 0.1, 2.0, 1.0, 1.0, 1].\npopulationsize and maxiter are options for the optimizer.\npopulationsize: the number of candidate solutions in each generation (only for :BBO)\nmaxiter: the maximum number of iterations\nThe lower bounds for q and nu0 are 0 and dP+2.\nThe upper bounds for q, nu0, and VAR lag can be set by upper_q, upper_nu0, and upper_p.\nThe default option for upper_nu0 is the time-series length of the data.\nIf you use the default option for mean_phi_const,\nmean_phi_const[dQ+1:end] is a zero vector.\nmean_phi_const[1:dQ] is calibrated to make the prior mean of λₚ a zero vector.\nAfter step 2, mean_phi_const[1] is replaced with mean_phi_const_PC1 if it is not empty.\nmean_phi_const = Matrix(your prior, dP, upper_p)\nmean_phi_const[:,i] is the prior mean for the VAR(i) constant. Therefore, mean_phi_const is a matrix only in this function. In other functions, mean_phi_const is a vector for the orthogonalized VAR system with the selected lag.\nWhen fix_const_PC1==true, the first element in the constant term in the orthogonalized VAR is fixed to its prior mean during posterior sampling.\ndata_scale::scalar: In a typical affine term structure model, theoretical yields are in decimals and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields and use (data_scale*theoretical yields) as the variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\nkappaQ_prior_pr is a vector of prior distributions for kappaQ under the JSZ model: each element specifies the prior for kappaQ[i] and must be provided as a Distributions.jl object. Alternatively, you can supply prior_mean_diff_kappaQ and prior_std_diff_kappaQ, which define means and standard deviations for Normal priors on [kappaQ[1]; diff(kappaQ)]; the implied Normal prior for each kappaQ[i] is then truncated to (0, 1). These options are only needed when using the JSZ model.\nis_pure_EH::Bool: When mean_phi_const=[], is_pure_EH=false sets mean_phi_const to zero vectors. Otherwise, mean_phi_const is set to imply the pure expectation hypothesis under mean_phi_const=[].\npsi_const is multiplied with the prior variance of the intercept coefficients in the orthogonalized transition equation.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.\n\nOutput(3)\n\nOptimized hyperparameter, optimization result, psi matrix\n\nThe second output contains optimization results: when optimizer=:LBFGS, a NamedTuple with minimizer, minimum, p, all_minimizer, all_minimum, selected_vars, psi; when optimizer=:BBO, a NamedTuple with opt (bboptimize result), selected_vars, psi. selected_vars is a sorted list of (lag, variable) tuples indicating which columns are included beyond the always-included columns 1:dQ.\nThe third output is psi (dP × dPp), the prior variance scaling matrix for lagged regressors in the orthogonalized transition equation. Backward variable selection is applied to all columns except lag-1 latent factors (j ≤ dQ, k = 1): setting psi[1:dQ, col] = 0 excludes a variable's effect on latent factors. For lag k, variable j, the column index is (k-1)dP+j.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Aₓ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.Aₓ","text":"Aₓ(aτ_, tau_n)\n\nInput\n\naτ_ is an output of function aτ.\n\nOutput\n\nAₓ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Aₚ-NTuple{4, Any}","page":"API","title":"TermStructureModels.Aₚ","text":"Aₚ(Aₓ_, Bₓ_, T0P_, Wₒ)\n\nInput\n\nAₓ_, Bₓ_, and T0P_ are outputs of function Aₓ, Bₓ, and T0P, respectively.\n\nOutput\n\nAₚ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Bₓ-Tuple{Any, Any}","page":"API","title":"TermStructureModels.Bₓ","text":"Bₓ(bτ_, tau_n)\n\nInput\n\nbτ_ is an output of function bτ.\n\nOutput\n\nBₓ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Bₚ-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.Bₚ","text":"Bₚ(Bₓ_, T1X_, Wₒ)\n\nInput\n\nBₓ_ and T1X_ are outputs of function Bₓ and T1X, respectively.\n\nOutput\n\nBₚ\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.Kphi-NTuple{4, Any}","page":"API","title":"TermStructureModels.Kphi","text":"Kphi(i, V, Xphi, dP)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.NIG_NIG-NTuple{6, Any}","page":"API","title":"TermStructureModels.NIG_NIG","text":"NIG_NIG(y, X, β₀, B₀, α₀, δ₀)\n\nNormal-InverseGamma-Normal-InverseGamma update\n\nprior: β|σ² ~ MvNormal(β₀,σ²B₀), σ² ~ InverseGamma(α₀,δ₀)\nlikelihood: y|β,σ² = Xβ + MvNormal(zeros(T,1),σ²I(T))\n\nOutput(2)\n\nβ, σ²\n\nposterior sample\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.PCs_2_latents-Tuple{Any, Any}","page":"API","title":"TermStructureModels.PCs_2_latents","text":"PCs_2_latents(yields, tau_n; kappaQ, kQ_infty, KPF, GPFF, OmegaFF, data_scale, pca_loadings=[])\n\nNotation XF is for the latent factor space and notation F is for the PC state space.\n\nInput\n\ndata_scale::scalar: In typical affine term structure models, theoretical yields are in decimal and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\n\nOutput(6)\n\nlatent, kappaQ, kQ_infty, KPXF, GPXFXF, OmegaXFXF\n\nLatent factors contain initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.S-Tuple{Any}","page":"API","title":"TermStructureModels.S","text":"S(i; Omega0)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.S_hat-NTuple{6, Any}","page":"API","title":"TermStructureModels.S_hat","text":"S_hat(i, m, V, yphi, Xphi, dP; Omega0)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.T0P-NTuple{4, Any}","page":"API","title":"TermStructureModels.T0P","text":"T0P(T1X_, Aₓ_, Wₚ, c)\n\nInput\n\nT1X_ and Aₓ_ are outputs of function T1X and Aₓ, respectively. c is a sample mean of PCs.\n\nOutput\n\nT0P\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.T1X-Tuple{Any, Any}","page":"API","title":"TermStructureModels.T1X","text":"T1X(Bₓ_, Wₚ)\n\nInput\n\nBₓ_ is an output of function Bₓ.\n\nOutput\n\nT1X\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._conditional_expectation-NTuple{6, Any}","page":"API","title":"TermStructureModels._conditional_expectation","text":"_conditional_expectation(S, τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale, pca_loadings)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._conditional_forecast-NTuple{6, Any}","page":"API","title":"TermStructureModels._conditional_forecast","text":"_conditional_forecast(S, τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale, pca_loadings)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._termPremium-NTuple{6, Any}","page":"API","title":"TermStructureModels._termPremium","text":"_termPremium(τ, PCs, macros, bτ_, T0P_, T1X_; kappaQ, kQ_infty, KPF, GPFF, ΩPP, data_scale)\n\nThis function calculates the term premium for maturity τ.\n\nInput\n\ndata_scale::scalar = In typical affine term structure models, theoretical yields are in decimal and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput(4)\n\nTP, timevarying_TP, const_TP, jensen\n\nTP: term premium of maturity τ\ntimevarying_TP: contributions of each [PCs macros] on TP at each time t (row: time, col: variable)\nconst_TP: Constant part of TP\njensen: Jensen's Inequality part in TP\nThe output excludes the time period for the initial observations.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._unconditional_expectation-NTuple{5, Any}","page":"API","title":"TermStructureModels._unconditional_expectation","text":"_unconditional_expectation(τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale, pca_loadings)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels._unconditional_forecast-NTuple{5, Any}","page":"API","title":"TermStructureModels._unconditional_forecast","text":"_unconditional_forecast(τ, horizon, yields, macros, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, mean_macros, data_scale, pca_loadings)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.aτ-NTuple{4, Any}","page":"API","title":"TermStructureModels.aτ","text":"aτ(N, bτ_, tau_n, Wₚ; kQ_infty, ΩPP, data_scale)\naτ(N, bτ_; kQ_infty, ΩXX, data_scale)\n\nThis function has two methods (multiple dispatch).\n\nInput\n\nWhen Wₚ ∈ arguments: This function calculates aτ using ΩPP.\nOtherwise: This function calculates aτ using ΩXX = OmegaXFXF[1:dQ, 1:dQ], so parameters are in the latent factor space and Wₚ is not needed.\nbτ_ is an output of function bτ.\ndata_scale::scalar: In typical affine term structure models, theoretical yields are in decimal and not annualized. However, for convenience (public data usually contains annualized percentage yields) and numerical stability, we sometimes want to scale up yields, so want to use (data_scale*theoretical yields) as variable yields. In this case, you can use the data_scale option. For example, we can set data_scale = 1200 and use annualized percentage monthly yields as yields.\n\nOutput\n\nVector(Float64)(aτ,N)\nFor the i-th maturity, Output[i] is the corresponding aτ.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.btau-Tuple{Any}","page":"API","title":"TermStructureModels.btau","text":"btau(N; kappaQ)\n\nThis function solves the difference equation for bτ in the closed form expression, assuming the distinct eigenvalues under the JSZ model.\n\nOutput\n\nFor maturity i, btau[:, i] is a vector of factor loadings.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.bτ-Tuple{Any}","page":"API","title":"TermStructureModels.bτ","text":"bτ(N; kappaQ, dQ)\n\nThis function solves the difference equation for bτ.\n\nOutput\n\nFor maturity i, bτ[:, i] is a vector of factor loadings.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.jensens_inequality-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.jensens_inequality","text":"jensens_inequality(τ, bτ_, T1X_; ΩPP, data_scale)\n\nThis function evaluates the Jensen's Inequality term. All terms are invariant with respect to the data_scale, except for this Jensen's inequality term, so the term needs to be scaled down by data_scale.\n\nOutput\n\nJensen's Inequality term for aτ of maturity τ.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_NUTS-NTuple{7, Any}","page":"API","title":"TermStructureModels.loglik_NUTS","text":"loglik_NUTS(i, yields, PCs, tau_n, macros, dims_phi, p; phiQ, varFFQ, diff_kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale, pca_loadings)\n\nThis function calculates the likelihood of the NUTS block.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_mea2-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.loglik_mea2","text":"loglik_mea2(yields, tau_n, p; kappaQ, kQ_infty, ΩPP, SigmaO, data_scale, pca_loadings)\n\nThis function is the same as loglik_mea but it requires ΩPP as an input.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.loglik_mea_NUTS-Tuple{Any, Any}","page":"API","title":"TermStructureModels.loglik_mea_NUTS","text":"loglik_mea_NUTS(yields, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale, pca_loadings)\n\nThis function generates the log likelihood of the measurement equation. It is used for posterior_NUTS.\n\nOutput\n\nthe measurement equation part of the log likelihood\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.logprior_C-Tuple{Any}","page":"API","title":"TermStructureModels.logprior_C","text":"logprior_C(C; Omega0::Vector)\n\nThis is a companion function of prior_C. It calculates the log density of the prior distribution for C.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.logprior_phi0-Tuple{Any, Any, Vector, Vararg{Any, 4}}","page":"API","title":"TermStructureModels.logprior_phi0","text":"logprior_phi0(phi0, mean_phi_const, rho::Vector, GQ_XX_mean, p, dQ, dP; psi_const, psi, q, nu0, Omega0, fix_const_PC1)\n\nThis is a companion function of prior_phi0. It calculates the log density of the prior distribution for phi0.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.logprior_varFF-Tuple{Any}","page":"API","title":"TermStructureModels.logprior_varFF","text":"logprior_varFF(varFF; nu0, Omega0::Vector)\n\nThis is a companion function of prior_varFF. It calculates the log density of the prior distribution for varFF.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.longvar-Tuple{Any}","page":"API","title":"TermStructureModels.longvar","text":"longvar(v)\n\nThis function calculates the long-run variance of v using the quadratic spectral window with bandwidth selection of Andrews (1991). The AR(1) approximation is used.\n\nInput\n\nTime-series vector v\n\nOutput\n\nEstimated 2πh(0) of v, where h(x) is the spectral density of v at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.minnesota-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.minnesota","text":"minnesota(l, i, j; q, nu0, Omega0, dQ=[])\n\nThis function returns the unscaled prior variance of the Minnesota prior.\n\nInput\n\nlag l, dependent variable i, regressor j in the VAR(p)\nq[:,1] and q[:,2] are [own, inner cross, outer cross, lag, intercept] shrinkages for the first dQ and remaining dP-dQ equations, respectively. Here, when the dependent variable is a principal component, inner cross refers to the other principal components (excluding itself), whereas outer cross refers to the macroeconomic variables. Likewise, when the dependent variable is a macroeconomic variable, inner cross refers to the other macroeconomic variables (excluding itself), whereas outer cross refers to the principal components.\nnu0(d.f.), Omega0(scale): Inverse-Wishart prior for the error-covariance matrix of VAR(p).\n\nOutput\n\nMinnesota part in the prior variance\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.mle_error_covariance-NTuple{4, Any}","page":"API","title":"TermStructureModels.mle_error_covariance","text":"mle_error_covariance(yields, macros, tau_n, p; pca_loadings=[])\n\nThis function calculates the MLE estimates of the error covariance matrix of the VAR(p) model.\n\npca_loadings=Matrix{, dQ, size(yields, 2)} stores the loadings for the first dQ principal components (so principal_components = yields * pca_loadings'), and you may optionally provide these loadings externally; if omitted, the package computes them internally via PCA.  ￼\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_hat-NTuple{6, Any}","page":"API","title":"TermStructureModels.phi_hat","text":"phi_hat(i, m, V, yphi, Xphi, dP)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.phi_varFF_2_ΩPP-Tuple{}","page":"API","title":"TermStructureModels.phi_varFF_2_ΩPP","text":"phi_varFF_2_ΩPP(; phi, varFF, dQ=[])\n\nThis function constructs ΩPP from statistical parameters.\n\nOutput\n\nΩPP\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_SigmaO-Tuple{Any, Any}","page":"API","title":"TermStructureModels.post_SigmaO","text":"post_SigmaO(yields, tau_n; kappaQ, kQ_infty, ΩPP, gamma, p, data_scale, pca_loadings)\n\nPosterior sampler for the measurement errors\n\nOutput\n\nVector{Dist}(IG, N-dQ)\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_gamma-Tuple{}","page":"API","title":"TermStructureModels.post_gamma","text":"post_gamma(; gamma_bar, SigmaO)\n\nPosterior sampler for the population measurement error\n\nOutput\n\nVector{Dist}(Gamma,length(SigmaO))\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kQ_infty-NTuple{4, Any}","page":"API","title":"TermStructureModels.post_kQ_infty","text":"post_kQ_infty(mean_kQ_infty, std_kQ_infty, yields, tau_n; kappaQ, phi, varFF, SigmaO, data_scale, pca_loadings)\n\nOutput\n\nFull conditional posterior distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kappaQ-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.post_kappaQ","text":"post_kappaQ(yields, prior_kappaQ_, tau_n; kQ_infty, phi, varFF, SigmaO, data_scale, pca_loadings)\n\nInput\n\nprior_kappaQ_ is an output of function prior_kappaQ.\n\nOutput\n\nFull conditional posterior distribution\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kappaQ2-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.post_kappaQ2","text":"post_kappaQ2(yields, prior_kappaQ_, tau_n; kappaQ, kQ_infty, phi, varFF, SigmaO, data_scale, x_mode, inv_x_hess, pca_loadings)\n\nThis function conducts the Metropolis-Hastings algorithm for the reparameterized kappaQ under the unrestricted JSZ form. x_mode and inv_x_hess constitute the mean and variance of the Normal proposal distribution.\n\nReparameterization:   kappaQ[1] = x[1]   kappaQ[2] = x[1] + x[2]   kappaQ[3] = x[1] + x[2] + x[3]\nJacobian:   [1 0 0   1 1 0   1 1 1]\nThe determinant = 1\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_kappaQ_phi_varFF_q_nu0-NTuple{8, Any}","page":"API","title":"TermStructureModels.post_kappaQ_phi_varFF_q_nu0","text":"post_kappaQ_phi_varFF_q_nu0(yields, macros, tau_n, mean_phi_const, rho, prior_q, prior_nu0, prior_diff_kappaQ; phi, psi, psi_const, varFF, q, nu0, kappaQ, kQ_infty, SigmaO, fix_const_PC1, data_scale, pca_loadings, sampler, chain, is_warmup)\n\nFull-conditional posterior sampler for kappaQ, phi and varFF\n\nInput\n\nprior_q: The 4 by 2 matrix that contains the prior distribution for q. All entries should be objects in Distributions.jl.\nprior_nu0: The prior distribution for nu0 - (dP + 1). It should be an object in Distributions.jl.\nprior_diff_kappaQ is a vector of the truncated normals(Distributions.truncated(Distributions.Normal(), lower, upper)). It has a prior for [kappaQ[1]; diff(kappaQ)].\nWhen fix_const_PC1==true, the first element in a constant term in the orthogonalized VAR is fixed to its prior mean during the posterior sampling.\nsampler and chain are the objects in Turing.jl.\nIf the current step is in the warmup phase, set is_warmup=true.\n\nOutput(6)\n\nchain, q, nu0, kappaQ, phi, varFF\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.post_phi_varFF-NTuple{6, Any}","page":"API","title":"TermStructureModels.post_phi_varFF","text":"post_phi_varFF(yields, macros, mean_phi_const, rho, prior_kappaQ_, tau_n; phi, psi, psi_const, varFF, q, nu0, Omega0, kappaQ, kQ_infty, SigmaO, fix_const_PC1, data_scale, pca_loadings)\n\nFull-conditional posterior sampler for phi and varFF\n\nInput\n\nprior_kappaQ_ is an output of function prior_kappaQ.\nWhen fix_const_PC1==true, the first element in a constant term in the orthogonalized VAR is fixed to its prior mean during the posterior sampling.\n\nOutput(3)\n\nphi, varFF, isaccept=Vector{Bool}(undef, dQ)\n\nReturns a posterior sample.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_C-Tuple{}","page":"API","title":"TermStructureModels.prior_C","text":"prior_C(; Omega0::Vector)\n\nThis function translates the Inverse-Wishart prior to a series of the Normal-Inverse-Gamma (NIG) prior distributions. If the dimension is dₚ, there are dₚ NIG prior distributions. This function generates Normal priors.\n\nOutput:\n\nunscaled prior of C in the LDLt decomposition, OmegaFF = inv(C)*diagm(varFF)*inv(C)'\n\nImportant note\n\nprior variance for C[i,:] = varFF[i]*variance of output[i,:]\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_gamma-Tuple{Any, Any}","page":"API","title":"TermStructureModels.prior_gamma","text":"prior_gamma(yields, p; pca_loadings)\n\nThere is a hierarchical structure in the measurement equation. The prior means of the measurement errors are gamma[i] and each gamma[i] follows Gamma(1,gamma_bar) distribution. This function decides gamma_bar empirically. OLS is used to estimate the measurement equation and then a variance of residuals is calculated for each maturity. An inverse of the average residual variances is set to gamma_bar.\n\nOutput\n\nhyperparameter gamma_bar\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_phi0-Tuple{Any, Vector, Any, Any, Any}","page":"API","title":"TermStructureModels.prior_phi0","text":"prior_phi0(mean_phi_const, rho::Vector, prior_kappaQ_, tau_n, Wₚ; psi_const, psi, q, nu0, Omega0, fix_const_PC1)\n\nThis function derives the prior distribution for coefficients of the lagged regressors in the orthogonalized VAR.\n\nInput\n\nprior_kappaQ_ is an output of function prior_kappaQ.\nWhen fix_const_PC1==true, the first element in a constant term in the orthogonalized VAR is fixed to its prior mean during the posterior sampling.\n\nOutput\n\nNormal prior distributions on the slope coefficient of lagged variables and intercepts in the orthogonalized equation.\nOutput[:,1] for intercepts, Output[:,1+1:1+dP] for the first lag, Output[:,1+dP+1:1+2*dP] for the second lag, and so on.\n\nImportant note\n\nprior variance for phi[i,:] = varFF[i]*var(output[i,:])\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.prior_varFF-Tuple{}","page":"API","title":"TermStructureModels.prior_varFF","text":"prior_varFF(; nu0, Omega0::Vector)\n\nThis function translates the Inverse-Wishart prior to a series of the Normal-Inverse-Gamma (NIG) prior distributions. If the dimension is dₚ, there are dₚ NIG prior distributions. This function generates Inverse-Gamma priors.\n\nOutput:\n\nprior of varFF in the LDLt decomposition,OmegaFF = inv(C)*diagm(varFF)*inv(C)'\nEach element in the output follows Inverse-Gamma priors.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.yphi_Xphi-Tuple{Any, Any, Any}","page":"API","title":"TermStructureModels.yphi_Xphi","text":"yphi_Xphi(PCs, macros, p)\n\nThis function generates the dependent variable and the corresponding regressors in the orthogonalized transition equation.\n\nOutput(4)\n\nyphi, Xphi = [ones(T - p) Xphi_lag Xphi_contemporaneous], [ones(T - p) Xphi_lag], Xphi_contemporaneous\n\nyphi and Xphi is a full matrix. For the i'th equation, the dependent variable is yphi[:,i] and the regressor is Xphi.\nXphi is the same for all orthogonalized transition equations. The orthogonalized equations are different in terms of contemporaneous regressors. Therefore, the corresponding regressors in Xphi should be excluded. The form of parameter phi performs that task by setting the coefficients of the excluded regressors to zeros. In particular, for the last dP by dP block in phi, the diagonals and the upper diagonal elements should be zero.\n\n\n\n\n\n","category":"method"},{"location":"api/#TermStructureModels.ν-Tuple{Any, Any}","page":"API","title":"TermStructureModels.ν","text":"ν(i, dP; nu0)\n\n\n\n\n\n","category":"method"},{"location":"others/#Other-Forms-of-the-Model","page":"Other Forms of the Model","title":"Other Forms of the Model","text":"","category":"section"},{"location":"others/#Yield-Only-Model","page":"Other Forms of the Model","title":"Yield-Only Model","text":"You may want to use yield-only models in which macros is an empty set. In such instances, set macros = [] and rho = [] for all functions.","category":"section"},{"location":"others/#Unrestricted-JSZ-model","page":"Other Forms of the Model","title":"Unrestricted JSZ model","text":"Our model is the three-factor JSZ(Joslin, Singleton, and Zhu, 2011) model. Under our default option, the JSZ model is constrained by the AFNS(Christensen, Diebold, and Rudebusch, 2011) restriction. Under this restriction, the eigenvalues of the risk-neutral slope matrix are [1, exp(-kappaQ), exp(-kappaQ)].\n\nThe package also allows you to estimate three distinct eigenvalues through an option. However, in this case, you must specify prior distributions for the three eigenvalues. These prior distributions should be in the form of Distribution objects from the Distributions.jl package. For example, the following prior distributions can be specified:\n\nkappaQ_prior_pr = [truncated(Normal(0.9, 0.05), -1, 1), truncated(Normal(0.9, 0.05), -1, 1), truncated(Normal(0.9, 0.05), -1, 1)]\n\nThe kappaQ_prior_pr containing the prior distributions is a vector of length 3, and each entry is an object from the Distributions.jl package. kappaQ_prior_pr[i] is the prior distribution of the i-th diagonal element of the slope matrix of the VAR system under the risk-neutral measure.\n\nAfter constructing kappaQ_prior_pr, it should be input as a keyword argument in functions that require the kappaQ_prior_pr variable (notably tuning_hyperparameter and posterior_sampler). If kappaQ_prior_pr is not provided, the model automatically operates under the AFNS constraint.\n\nNote that when setting prior distributions, \"the prior expectation of the slope matrix[1:3, 1:3] of the first lag of the VAR model under the physical measure\" is set to diagm(mean.(kappaQ_prior_pr)) under the unrestricted JSZ model. Therefore, the prior distributions of the eigenvalues should be set to reflect the prior expectations under the physical measure to some extent.\n\nThe notation for the three eigenvalues is kappaQ. Therefore, kappaQ is a three-dimensional vector under the unrestricted JSZ model. In contrast, kappaQ is a scalar that represents the DNS decay parameter under the AFNS restriction.","category":"section"},{"location":"estimation/#Estimation","page":"Estimation","title":"Estimation","text":"To estimate the model, the following two steps must be undertaken.","category":"section"},{"location":"estimation/#Step-1.-Tuning-Hyperparameters","page":"Estimation","title":"Step 1. Tuning Hyperparameters","text":"We have five hyperparameters, p, q, nu0, Omega0, and mean_phi_const.\n\np::Float64: lag length of the mathbbP-VAR(p)\nq::Matrix{Float64}( , 5, 2): Shrinkage degrees in the Minnesota prior\nnu0::Float64(d.f.) and Omega0::Vector(diagonals of the scale matrix): Prior distribution of the error covariance matrix in the mathbbP-VAR(p)\nmean_phi_const: Prior mean of the intercept term in the mathbbP-VAR(p)\n\nWe recommend tuning_hyperparameter for deciding the hyperparameters.\n\ntuned, results = tuning_hyperparameter(yields, macros, tau_n, rho; populationsize=50, maxiter=10_000, medium_tau=collect(24:3:48), upper_q=[1 1; 1 1; 1 1; 4 4; 100 100], mean_kQ_infty=0, std_kQ_infty=0.1, upper_nu0=[], mean_phi_const=[], fix_const_PC1=false, upper_p=24, mean_phi_const_PC1=[], data_scale=1200, kappaQ_prior_pr=[], init_nu0=[], is_pure_EH=false, psi_common=[], psi_const=[], pca_loadings=[], prior_mean_diff_kappaQ=[], prior_std_diff_kappaQ=[], optimizer=:LBFGS, ml_tol=1.0, init_x=[])\n\nNote that the default upper bound of p is upper_p=24. The output tuned::Hyperparameter is the object that should be obtained in Step 1. results contains the optimization results.\n\nIf you accept the default values, the function is simplified to\n\ntuned, results = tuning_hyperparameter(yields, macros, tau_n, rho)\n\nyields is a T by N matrix, T is the length of the sample period and N is the number of maturities in the data. tau_n is an N-Vector that contains bond maturities in the data. For example, if there are two maturities, 3 and 24 months, in the monthly term structure model, tau_n=[3; 24]. macros is a T by dP-dQ matrix in which each column represents an individual macroeconomic variable. rho is a dP-dQ-Vector. In general, rho[i] = 1 if macros[:, i] is in levels, or it is set to 0 if the macro variable is differenced.","category":"section"},{"location":"estimation/#Several-relevant-points-regarding-hyperparameter-optimization","page":"Estimation","title":"Several relevant points regarding hyperparameter optimization","text":"","category":"section"},{"location":"estimation/#Optimization-Algorithms","page":"Estimation","title":"Optimization Algorithms","text":"We provide two optimization algorithms via the optimizer option:\n\n:LBFGS (default, recommended): Uses gradient-based LBFGS optimization from Optim.jl. This algorithm alternates between optimizing hyperparameters (with fixed lag) and selecting the best lag (with fixed hyperparameters) until convergence. It is fast and efficient, making it suitable for most applications. However, it does not guarantee finding the global optimum due to its local search nature.\n:BBO: Uses a Differential Evolutionary (DE) algorithm from BlackBoxOptim.jl. This algorithm optimizes hyperparameters and lag simultaneously and is more likely to find the global optimum. The downside is that it has higher computational costs and lacks automatic convergence detection—users must verify convergence by examining the objective function values or setting a sufficient number of iterations via maxiter.\n\nWe recommend using the default :LBFGS optimizer for most cases due to its speed and reliability. If you suspect the optimization is stuck in a local optimum or need to verify global optimality, you can use :BBO with the populationsize and maxiter options adjusted according to your computational budget.","category":"section"},{"location":"estimation/#Range-of-Data-over-which-the-Marginal-Likelihood-is-Calculated","page":"Estimation","title":"Range of Data over which the Marginal Likelihood is Calculated","text":"In Bayesian methodology, the standard criterion for model comparison is the marginal likelihood. When comparing models using the marginal likelihood, the most crucial prerequisite is that the marginal likelihoods of all models must be calculated over the same observations.\n\nFor instance, suppose we have data with 100 rows. Model 1 has p=1, and Model 2 has p=2. In this case, the marginal likelihood should be computed over data[3:end, :]. This means that for Model 1, data[2, :] is used as the initial value, and for Model 2, data[1:2, :] are used as initial values. tuning_hyperparameter automatically accounts for this by calculating the marginal likelihood over data[upper_p+1:end, :] for model comparison.","category":"section"},{"location":"estimation/#Prior-Belief-about-the-Expectation-Hypothesis","page":"Estimation","title":"Prior Belief about the Expectation Hypothesis","text":"The algorithm has an inductive bias that the estimates should not deviate too much from the Expectation Hypothesis (EH). Here, the assumed EH means that the term premium is a non-zero constant. If you want to introduce an inductive bias centered around the pure EH, where the term premium is zero, set is_pure_EH=true. However, note that using this option may take some additional time to numerically set the prior distribution.","category":"section"},{"location":"estimation/#Step-2.-Sampling-the-Posterior-Distribution-of-Parameters","page":"Estimation","title":"Step 2. Sampling the Posterior Distribution of Parameters","text":"In Step 1, we obtained tuned::Hyperparameter. posterior_sampler uses it for the estimation.\n\nsaved_params, acceptPrMH = posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter; medium_tau=collect(24:3:48), init_param=[], psi=[], psi_const=[], gamma_bar=[], kappaQ_prior_pr=[], mean_kQ_infty=0, std_kQ_infty=0.1, fix_const_PC1=false, data_scale=1200, pca_loadings=[], kappaQ_proposal_mode=[])\n\nIf you changed the default values in Step 1, the corresponding default values in the above function should also be changed. If you use the default values, the function simplifies to\n\nsaved_params, acceptPrMH = posterior_sampler(yields, macros, tau_n, rho, iteration, tuned::Hyperparameter)\n\niteration is the number of posterior samples to generate. The MCMC sampler starts at the prior mean, and you need to discard burn-in samples manually.\n\nsaved_params::Vector{Parameter} has length iteration, and each entry is a posterior sample. acceptPrMH is a dQ+1-Vector, where the i(<=dQ)-th entry shows the MH acceptance rate for the i-th principal component in the recursive mathbbP-VAR. The last entry of acceptPrMH is the MH acceptance rate for kappaQ under the unrestricted JSZ model. It is zero under the AFNS restriction.","category":"section"},{"location":"estimation/#Step-3.-Discard-Burn-in-and-Nonstationary-Posterior-Samples","page":"Estimation","title":"Step 3. Discard Burn-in and Nonstationary Posterior Samples","text":"After obtaining posterior samples (saved_params), you may want to discard some samples as burn-in. If the number of burn-in samples is burnin, run\n\nsaved_params = saved_params[burnin+1:end]\n\nYou may also want to discard posterior samples that do not satisfy the stationarity condition. This can be done using erase_nonstationary_param.\n\nsaved_params, Pr_stationary = erase_nonstationary_param(saved_params; threshold=1)\n\nAll entries in the output (saved_params::Vector{Parameter}) are posterior samples that satisfy the stationarity condition.\n\nwarning: Reduction in the Number of Posterior Samples\nThe length of saved_params decreases after the burn-in process and applying erase_nonstationary_param. Note that this creates a gap between iteration and length(saved_params).\n\nnote: Handling Non-Stationary Data\nThe optional input threshold is designed to discard posterior samples with eigenvalues of the VAR system exceeding the specified threshold. Traditionally, we use a stationary VAR, so the default threshold is set to 1. However, for non-stationary VAR models, it may be necessary to allow for a slightly higher threshold. In such cases, you can set threshold to a value greater than 1, such as 1.05.","category":"section"},{"location":"estimation/#Diagnostics-for-MCMC","page":"Estimation","title":"Diagnostics for MCMC","text":"We believe in the efficiency of the algorithm, so you do not need to be overly concerned about the convergence of the posterior samples. In our opinion, sampling 6,000 posterior samples and discarding the first 1,000 samples as burn-in should be sufficient.\n\nWe provide a measure to gauge the efficiency of the algorithm, that is\n\nineff = ineff_factor(saved_params)\n\nsaved_params::Vector{Parameter} is the output of posterior_sampler. ineff is a Tuple(kappaQ, kQ_infty, gamma, SigmaO, varFF, phi). Each object in the tuple has the same shape as its corresponding parameter. The entries in the Array of the Tuple represent the inefficiency factors of the corresponding parameters. If an inefficiency factor is high, it indicates poor sampling efficiency for the parameter at that position.\n\nYou can calculate the maximum inefficiency factor by\n\nmax_ineff = (ineff[1] |> maximum, ineff[2], ineff[3] |> maximum, ineff[4] |> maximum, ineff[5] |> maximum, ineff[6] |> maximum) |> maximum\n\nThe value obtained by dividing the number of posterior samples by max_ineff is the effective number of posterior samples, accounting for the efficiency of the sampler. For example, suppose max_ineff = 10. If 6,000 posterior samples are drawn and the first 1,000 samples are discarded as burn-in, the remaining 5,000 posterior samples have the same efficiency as 500 i.i.d. samples, calculated as (6000-1000)/max_ineff.","category":"section"},{"location":"notations/#Notations","page":"Notations","title":"Notations","text":"This package is based on the paper. The variable names in the package and the notation in the paper correspond as follows.\n\n(Image: Notation Table)","category":"section"},{"location":"#TermStructureModels.jl","page":"Overview","title":"TermStructureModels.jl","text":"TermStructureModels.jl provides the following functions.\n\nStatistical Inference\nParameters\nYield Curve Interpolation\nTerm Premiums\nForecasting\nConditional Forecasting without scenarios (Baseline Forecasts)\nScenario Analysis (Scenario Forecasts)\n\nTo use the above functions, you must first estimate the model. Use posterior_sampler to obtain posterior samples of parameters. These posterior samples are then used for the above functions (statistical inference and forecasting). For details, refer to the corresponding pages.\n\nSome outputs of our package are not simple arrays. They are\n\nVector{Parameter}: output of posterior_sampler\nVector{ReducedForm}: output of reducedform\nVector{LatentSpace}: output of latentspace\nVector{YieldCurve}: output of fitted_yieldcurve\nVector{TermPremium}: output of term_premium\nVector{Forecast}: outputs of conditional_forecast and conditional_expectation\n\nThe above outputs contain information about the posterior distributions of objects of interest. You can use these outputs to extract posterior samples or calculate descriptive statistics of the posterior distributions.\n\nThis package is based on our paper. Descriptions of the model and the meanings of each variable can be found in the paper. The Notation section details how notations in the paper correspond to variables in the package. Additionally, the example file used in the paper is available in the repository.\n\nYou are encouraged to read the two text boxes below.\n\nwarning: Unit of Data\nTheoretical term structure models typically describe bond yields as decimals per time period. However, yield data is typically presented in percent per annum. Therefore, you need to address this discrepancy using the data_scale option. data_scale represents the scale of the data. Specifically,`yields_in_data` = `data_scale`*`theoretical_yields_in_the_model`holds. For example, suppose we have monthly yield data in percent per annum. If we use a monthly term structure model, then data_scale=1200. The default value of data_scale is 1200 for all functions.Functions that have the data_scale option are as follows:tuning_hyperparameter\nposterior_sampler\nterm_premium\nconditional_forecast\nconditional_expectation\nlatentspace\nreducedform\nfitted_yieldcurve\ngenerative\ncalibrate_mean_phi_const\n\ntip: Normalization of Data\nThe package demeans the principal components of bond yields, which are spanned risk factors in the bond market. Therefore, we recommend demeaning macro data before use. Note that demeaning the macro variables is recommended but not mandatory.","category":"section"},{"location":"inference/#Statistical-Inference","page":"Statistical Inference","title":"Statistical Inference","text":"","category":"section"},{"location":"inference/#Inference-for-Parameters","page":"Statistical Inference","title":"Inference for Parameters","text":"You can obtain posterior samples of the term structure model parameters using reducedform.\n\nreduced_params = reducedform(saved_params, yields, macros, tau_n; data_scale=1200, pca_loadings=[])\n\nyields is a T by N matrix, T is the length of the sample period and N is the number of bond maturities in the data. tau_n is an N-Vector that contains maturities in the data. For example, if there are two maturities, 3 and 24 months, in the monthly term structure model, tau_n=[3; 24]. macros is a T by dP-dQ matrix in which each column represents an individual macroeconomic variable.\n\nnote: Reason Why You Need to Run `reducedform` in Addition to `posterior_sampler`\nWe estimate the mathbbP-VAR by transforming it into a recursive VAR form. Therefore, Parameter, the output of posterior_sampler, contains parameters in the recursive VAR. In contrast, ReducedForm, the output of reducedform, contains parameters in the original reduced-form mathbbP-VAR.\n\nEach entry in reduced_params::Vector{ReducedForm} is a joint posterior sample of the parameters.","category":"section"},{"location":"inference/#Yield-Curve-Interpolation","page":"Statistical Inference","title":"Yield Curve Interpolation","text":"First, transform the parameter space from the principal component space to the latent factor space. This is done using latentspace. Then, use fitted_yieldcurve to obtain fitted yields. Specifically,\n\nsaved_latent_params = latentspace(saved_params, yields, tau_n; data_scale=1200, pca_loadings=[])\nfitted_yields = fitted_yieldcurve(tau_vec, saved_latent_params::Vector{LatentSpace}; data_scale=1200)\n\ntau_vec is a vector containing the maturities for which you want to calculate fitted yields through interpolation. fitted_yields::Vector{YieldCurve} contains the interpolation results.","category":"section"},{"location":"inference/#Term-Premiums","page":"Statistical Inference","title":"Term Premiums","text":"term_premium calculates the term premium of the bonds. tau_interest contains the maturities of interest and should be a Vector (at least a one-dimensional vector).\n\nsaved_TP, saved_tv_TP, saved_tv_EH = term_premium(tau_interest, tau_n, saved_params, yields, macros; data_scale=1200)\n\nsaved_TP::Vector{TermPremium} contains the results of the term premium calculations. Both the term premiums and expectation hypothesis components are decomposed into time-invariant and time-varying parts. For the maturity tau_interest[i], the time-varying parts are saved in saved_tv_TP[:, :, i] and saved_tv_EH[:, :, i]. The time-varying parts driven by the j-th pricing factor are stored in saved_tv_TP[:, j, i] and saved_tv_EH[:, j, i].","category":"section"},{"location":"output/#How-to-Utilize-the-Outputs-of-Functions","page":"Utilization of the Output","title":"How to Utilize the Outputs of Functions","text":"When you execute some functions, the output is Vector{<:PosteriorSample}. Examples of Vector{<:PosteriorSample} include\n\nVector{Parameter}: output of posterior_sampler\nVector{ReducedForm}: output of reducedform\nVector{LatentSpace}: output of latentspace\nVector{YieldCurve}: output of fitted_yieldcurve\nVector{TermPremium}: output of term_premium\nVector{Forecast}: outputs of conditional_forecast and conditional_expectation\n\nEach entry in the vectors above is a posterior sample and takes the form of a struct: Parameter, ReducedForm, LatentSpace, YieldCurve, TermPremium, or Forecast. The above six structs have unique fields. See the API section for the fields each struct contains. The Notations section explains the specific meanings of the fields.","category":"section"},{"location":"output/#Extract-Posterior-Samples-of-the-Fields","page":"Utilization of the Output","title":"Extract Posterior Samples of the Fields","text":"Vector{<:PosteriorSample} contains posterior samples of the fields of the corresponding struct. You can access posterior samples of a specific field by using getindex. For example, if you want to get posterior samples of phi in Parameter, run\n\nsamples_phi = saved_params[:phi]\n\nfor saved_params::Vector{Parameter}. Then samples_phi is a vector, and samples_phi[i] is the i-th posterior sample of phi. Note that samples_phi[i] is a matrix in this case. (Julia allows vectors to have array elements.)","category":"section"},{"location":"output/#Descriptive-Statistics-of-the-Posterior-Distributions","page":"Utilization of the Output","title":"Descriptive Statistics of the Posterior Distributions","text":"The package extends mean, var, std, median, and quantile from Statistics.jl to Vector{<:PosteriorSample}. These five functions can be used to calculate descriptive statistics of the posterior distribution, such as the posterior mean or posterior variance. For example, the posterior mean of phi can be calculated by\n\nmean_phi = mean(saved_params)[:phi]\n\nmean_phi[i,j] is the posterior mean of the entry in the i-th row and j-th column of phi. The outputs of all functions (mean, var, std, median, and quantile) have the same shapes as their corresponding parameters. quantile needs a second input. For example, in the case of\n\nq_phi = quantile(saved_params, 0.4)[:phi]\n\n40% of posterior samples of phi[i,j] are less than q_phi[i,j].\n\ntip: Tip\nTo get posterior samples or posterior descriptive statistics of a specific object, you need to know which struct contains the object as a field. The Notations section organizes which structs contain the object.","category":"section"}]
}
